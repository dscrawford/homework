{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Made by Daniel Crawford\n",
    "# Student Net ID: dsc160130\n",
    "# Course: CS6364 - Artificial Intelligence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "DISCOUNT_FACTOR = 0.7\n",
    "NEGATIVE_REWARD = -5\n",
    "INITIAL_STATE_PROB = 1 / 25\n",
    "\n",
    "grid = np.array([\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "\n",
    "class GridGame2D:\n",
    "    def __init__(self, grid):\n",
    "        self.grid = grid\n",
    "        self.x, self.y = grid.shape\n",
    "        self.num_actions = 4\n",
    "        self.terminal_states = {tuple(x) for x in np.argwhere(grid == 1)}\n",
    "\n",
    "        self.actions = {\n",
    "            0: lambda x, y: (x + 1, y) if x < self.x - 1 else (x, y),\n",
    "            1: lambda x, y: (x - 1, y) if x > 0 else (x, y),\n",
    "            2: lambda x, y: (x, y - 1) if y > 0 else (x, y),\n",
    "            3: lambda x, y: (x, y + 1) if y < self.y - 1 else (x, y)\n",
    "        }\n",
    "\n",
    "    def action(self, x, y, a):\n",
    "        if a > self.num_actions:\n",
    "            raise Exception('Invalid Action')\n",
    "\n",
    "        return self.actions[a](x, y)\n",
    "\n",
    "    def get_index(self, x, y):\n",
    "        return x * (self.x * y)\n",
    "\n",
    "    def get_action_str(self, action):\n",
    "        if action == 0:\n",
    "            return 'right'\n",
    "        elif action == 1:\n",
    "            return 'left'\n",
    "        elif action == 2:\n",
    "            return 'up'\n",
    "        else:\n",
    "            return 'down'\n",
    "\n",
    "\n",
    "action_num = 1\n",
    "\n",
    "\n",
    "class GridLearner2D:\n",
    "    def __init__(self, gamma, r):\n",
    "        self.gamma = gamma\n",
    "        self.r = r\n",
    "        self.Q = None\n",
    "\n",
    "    def train(self, game):\n",
    "        Q = np.zeros((game.x * game.y, 4))\n",
    "        while True:\n",
    "            newQ = np.zeros(Q.shape)\n",
    "            for y in range(game.y):\n",
    "                for x in range(game.x):\n",
    "                    for a in range(game.num_actions):\n",
    "                        newX, newY = game.action(x, y, a)\n",
    "                        if (newX, newY) in game.terminal_states:\n",
    "                            newQ[x + (y * game.x)][a] = self.r\n",
    "                        else:\n",
    "                            newQ[x + (y * game.x)][a] = self.r + self.gamma * np.max(Q[newX + (newY * game.x)])\n",
    "            if np.all(newQ == Q):\n",
    "                break\n",
    "            Q = newQ\n",
    "        self.Q = Q\n",
    "\n",
    "    def get_sequence(self, game, Q, x, y):\n",
    "        action_num = {'value': 1}\n",
    "\n",
    "        def get_sequence(game, Q, x, y, str=''):\n",
    "            if (x, y) in game.terminal_states:\n",
    "                str += 'END'\n",
    "                print('action ', action_num['value'], ':', str)\n",
    "                action_num['value'] += 1\n",
    "            else:\n",
    "                index = x + (y * game.x)\n",
    "                actions = np.ndarray.flatten(np.argwhere(Q[index] == np.max(Q[index])))\n",
    "                for i, action in enumerate(actions):\n",
    "                    subX, subY = game.action(x, y, action)\n",
    "                    new_str = str + game.get_action_str(action) + ' --> '\n",
    "                    get_sequence(game, Q, subX, subY, new_str)\n",
    "\n",
    "        get_sequence(game, Q, x, y)\n",
    "\n",
    "\n",
    "class DeepGridLearner2D(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DeepGridLearner2D, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_actions)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            y = layer(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, game, x, y, action):\n",
    "        newX, newY = game.action(x, y, action)\n",
    "        if self.capacity < len(self.memory):\n",
    "            self.memory[self.position] = [x, y, action, newX, newY]\n",
    "            self.position = (self.position + 1) % self.capacity\n",
    "        self.memory.append([x, y, action, newX, newY])\n",
    "\n",
    "    def sample(self, size):\n",
    "        return np.array(self.memory)[np.random.randint(0, len(self.memory), size)]\n",
    "\n",
    "\n",
    "def generate_coordinates(game):\n",
    "    x, y = np.random.randint(0, game.x), np.random.randint(0, game.y)\n",
    "    while (x, y) in game.terminal_states:\n",
    "        x, y = np.random.randint(0, game.x), np.random.randint(0, game.y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def q1():\n",
    "    game = GridGame2D(grid)\n",
    "    grid_learner = GridLearner2D(DISCOUNT_FACTOR, NEGATIVE_REWARD)\n",
    "    grid_learner.train(game)\n",
    "    for y in range(game.y):\n",
    "        for x in range(game.x):\n",
    "            index = x + game.x * y\n",
    "            max_points = np.ndarray.flatten(np.argwhere(grid_learner.Q[index] == np.max(grid_learner.Q[index])))\n",
    "            action_str = ''\n",
    "            for action in max_points:\n",
    "                if action == 0:\n",
    "                    action_str += '→'\n",
    "                elif action == 1:\n",
    "                    action_str += '←'\n",
    "                elif action == 2:\n",
    "                    action_str += '↑'\n",
    "                elif action == 3:\n",
    "                    action_str += '↓'\n",
    "            print(action_str, end='\\t')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "def q2(num_episodes=100, sequence_limit=5, target_update=10, batch_size=32, eps=1e-2):\n",
    "    game = GridGame2D(grid)\n",
    "    gamma = DISCOUNT_FACTOR\n",
    "    r = NEGATIVE_REWARD\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    replay = ReplayMemory(1000)\n",
    "\n",
    "    policy_model = DeepGridLearner2D(game.num_actions).to(device)\n",
    "    target_model = DeepGridLearner2D(game.num_actions).to(device)\n",
    "    target_model.eval()\n",
    "    target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.SGD(policy_model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    tq = tqdm(range(num_episodes))\n",
    "\n",
    "    for episode in tq:\n",
    "        x, y = generate_coordinates(game)\n",
    "        sequence_i = 0\n",
    "        while (x, y) not in game.terminal_states:\n",
    "            sequence_i += 1\n",
    "            with torch.no_grad():\n",
    "                if sequence_i < sequence_limit:\n",
    "                    action = np.random.randint(game.num_actions)\n",
    "            newX, newY = game.action(x, y, action)\n",
    "\n",
    "            states_np = np.array([generate_coordinates(game) for _ in range(batch_size)])\n",
    "            actions_np = np.array([np.random.randint(0, game.num_actions) for _ in range(batch_size)])\n",
    "            new_states_np = []\n",
    "            for i, (x_i, y_i) in enumerate(states_np):\n",
    "                new_states_np.append(game.action(x_i, y_i, actions_np[i]))\n",
    "            new_states_np = np.array(new_states_np)\n",
    "\n",
    "            states = torch.Tensor(states_np).float().to(device)\n",
    "            actions = torch.Tensor(actions_np).view(batch_size, 1).type(torch.int64).to(device)\n",
    "            new_states = torch.Tensor(new_states_np).float().to(device)\n",
    "\n",
    "            q_0 = policy_model(states).gather(1, actions)\n",
    "\n",
    "            expected_value = torch.full((batch_size, ), r, dtype=torch.float64).to(device)\n",
    "            non_terminal_mask = torch.Tensor(\n",
    "                [(int(new_states[i][0].cpu()), int(new_states[i][1].cpu())) not in game.terminal_states for i in range(batch_size)]\n",
    "            ).bool().to(device)\n",
    "            expected_value[non_terminal_mask] += target_model(new_states[non_terminal_mask]).max(1)[0] * gamma\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(q_0, expected_value.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            x, y = newX, newY\n",
    "        if (episode + 1) % target_update == 0:\n",
    "            target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y in range(game.y):\n",
    "            for x in range(game.x):\n",
    "                output = policy_model(torch.from_numpy(np.array([[x, y]])).float().to(device)).cpu().numpy()\n",
    "                output = np.ndarray.flatten(output)\n",
    "                max_points = np.ndarray.flatten(np.argwhere(output == np.max(output)))\n",
    "                action_str = ''\n",
    "                for action in max_points:\n",
    "                    if action == 0:\n",
    "                        action_str += '→'\n",
    "                    elif action == 1:\n",
    "                        action_str += '←'\n",
    "                    elif action == 2:\n",
    "                        action_str += '↑'\n",
    "                    elif action == 3:\n",
    "                        action_str += '↓'\n",
    "                print(action_str, end='\\t')\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "def q3(num_episodes=100, sequence_limit=5, target_update=10, batch_size=32, eps=1e-2):\n",
    "    game = GridGame2D(grid)\n",
    "    gamma = DISCOUNT_FACTOR\n",
    "    r = NEGATIVE_REWARD\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    replay = ReplayMemory(1000)\n",
    "\n",
    "    policy_model = DeepGridLearner2D(game.num_actions).to(device)\n",
    "    target_model = DeepGridLearner2D(game.num_actions).to(device)\n",
    "    target_model.eval()\n",
    "    target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.SGD(policy_model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    tq = tqdm(range(num_episodes))\n",
    "\n",
    "    for episode in tq:\n",
    "        x, y = generate_coordinates(game)\n",
    "        sequence_i = 0\n",
    "        while (x, y) not in game.terminal_states:\n",
    "            sequence_i += 1\n",
    "            s = torch.Tensor([[x, y]]).float().to(device)\n",
    "            with torch.no_grad():\n",
    "                if np.random.uniform() > eps and sequence_i < sequence_limit:\n",
    "                    _, action = policy_model(s).max(1)\n",
    "                    action = int(action.cpu())\n",
    "                else:\n",
    "                    sequence_i = 0\n",
    "                    action = np.random.randint(game.num_actions)\n",
    "            newX, newY = game.action(x, y, action)\n",
    "\n",
    "            replay.push(game, x, y, action)\n",
    "\n",
    "            batch = replay.sample(batch_size)\n",
    "\n",
    "            states = torch.Tensor(batch[:, 0:2]).float().to(device)\n",
    "            actions = torch.Tensor(batch[:, 2:3]).view(batch_size, 1).type(torch.int64).to(device)\n",
    "            new_states = torch.Tensor(batch[:, 3:5]).float().to(device)\n",
    "\n",
    "            q_0 = policy_model(states).gather(1, actions)\n",
    "\n",
    "            expected_value = torch.full((batch_size, ), r, dtype=torch.float64).to(device)\n",
    "            non_terminal_mask = torch.Tensor(\n",
    "                [(int(new_states[i][0].cpu()), int(new_states[i][1].cpu())) not in game.terminal_states for i in range(batch_size)]\n",
    "            ).bool().to(device)\n",
    "            expected_value[non_terminal_mask] += target_model(new_states[non_terminal_mask]).max(1)[0] * gamma\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(q_0, expected_value.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            x, y = newX, newY\n",
    "        if (episode + 1) % target_update == 0:\n",
    "            target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y in range(game.y):\n",
    "            for x in range(game.x):\n",
    "                output = policy_model(torch.from_numpy(np.array([[x, y]])).float().to(device)).cpu().numpy()\n",
    "                output = np.ndarray.flatten(output)\n",
    "                max_points = np.ndarray.flatten(np.argwhere(output == np.max(output)))\n",
    "                action_str = ''\n",
    "                for action in max_points:\n",
    "                    if action == 0:\n",
    "                        action_str += '→'\n",
    "                    elif action == 1:\n",
    "                        action_str += '←'\n",
    "                    elif action == 2:\n",
    "                        action_str += '↑'\n",
    "                    elif action == 3:\n",
    "                        action_str += '↓'\n",
    "                print(action_str, end='\\t')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1\n",
      "←↑\t←\t←\t←\t←↓\t\n",
      "\n",
      "↑\t←↑\t←↑\t→←↑↓\t↓\t\n",
      "\n",
      "↑\t←↑\t→←↑↓\t→↓\t↓\t\n",
      "\n",
      "↑\t→←↑↓\t→↓\t→↓\t↓\t\n",
      "\n",
      "→↑\t→\t→\t→\t→↓\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Question 1')\n",
    "q1()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 124.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2\n",
      "←\t←\t←\t←\t←\t\n",
      "\n",
      "↑\t←\t←\t←\t←\t\n",
      "\n",
      "↑\t↑\t←\t↑\t↑\t\n",
      "\n",
      "↑\t←\t←\t←\t↓\t\n",
      "\n",
      "↑\t↑\t←\t→\t→\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Question 2')\n",
    "q2(1000, 5, 10, 16, 1e-2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:23<00:00, 42.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3\n",
      "↑\t←\t←\t←\t←\t\n",
      "\n",
      "↑\t↑\t←\t←\t←\t\n",
      "\n",
      "↑\t↑\t↑\t↑\t↑\t\n",
      "\n",
      "↑\t↑\t←\t←\t→\t\n",
      "\n",
      "↑\t↑\t←\t→\t→\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Question 3')\n",
    "q3(1000, 5, 10, 16, 1e-2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}