{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Made by Daniel Crawford\n",
    "# Student Net ID: dsc160130\n",
    "# Course: CS6364 - Artificial Intelligence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "DISCOUNT_FACTOR = 0.7\n",
    "NEGATIVE_REWARD = -5\n",
    "INITIAL_STATE_PROB = 1 / 25\n",
    "\n",
    "grid = np.array([\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "\n",
    "class GridGame2D:\n",
    "    def __init__(self, grid):\n",
    "        self.grid = grid\n",
    "        self.x, self.y = grid.shape\n",
    "        self.num_actions = 4\n",
    "        self.terminal_states = {tuple(x) for x in np.argwhere(grid == 1)}\n",
    "\n",
    "        self.actions = {\n",
    "            0: lambda x, y: (x + 1, y) if x < self.x - 1 else (x, y),\n",
    "            1: lambda x, y: (x - 1, y) if x > 0 else (x, y),\n",
    "            2: lambda x, y: (x, y - 1) if y > 0 else (x, y),\n",
    "            3: lambda x, y: (x, y + 1) if y < self.y - 1 else (x, y)\n",
    "        }\n",
    "\n",
    "    def action(self, x, y, a):\n",
    "        if a > self.num_actions:\n",
    "            raise Exception('Invalid Action')\n",
    "\n",
    "        return self.actions[a](x, y)\n",
    "\n",
    "    def get_index(self, x, y):\n",
    "        return x * (self.x * y)\n",
    "\n",
    "    def get_action_str(self, action):\n",
    "        if action == 0:\n",
    "            return 'right'\n",
    "        elif action == 1:\n",
    "            return 'left'\n",
    "        elif action == 2:\n",
    "            return 'up'\n",
    "        else:\n",
    "            return 'down'\n",
    "\n",
    "\n",
    "action_num = 1\n",
    "\n",
    "\n",
    "class GridLearner2D:\n",
    "    def __init__(self, gamma, r):\n",
    "        self.gamma = gamma\n",
    "        self.r = r\n",
    "        self.Q = None\n",
    "\n",
    "    def train(self, game):\n",
    "        Q = np.zeros((game.x * game.y, 4))\n",
    "        while True:\n",
    "            newQ = np.zeros(Q.shape)\n",
    "            for y in range(game.y):\n",
    "                for x in range(game.x):\n",
    "                    for a in range(game.num_actions):\n",
    "                        newX, newY = game.action(x, y, a)\n",
    "                        if (newX, newY) in game.terminal_states:\n",
    "                            newQ[x + (y * game.x)][a] = self.r\n",
    "                        else:\n",
    "                            newQ[x + (y * game.x)][a] = self.r + self.gamma * np.max(Q[newX + (newY * game.x)])\n",
    "            if np.all(newQ == Q):\n",
    "                break\n",
    "            Q = newQ\n",
    "        self.Q = Q\n",
    "\n",
    "    def get_sequence(self, game, Q, x, y):\n",
    "        action_num = {'value': 1}\n",
    "\n",
    "        def get_sequence(game, Q, x, y, str=''):\n",
    "            if (x, y) in game.terminal_states:\n",
    "                str += 'END'\n",
    "                print('action ', action_num['value'], ':', str)\n",
    "                action_num['value'] += 1\n",
    "            else:\n",
    "                index = x + (y * game.x)\n",
    "                actions = np.ndarray.flatten(np.argwhere(Q[index] == np.max(Q[index])))\n",
    "                for i, action in enumerate(actions):\n",
    "                    subX, subY = game.action(x, y, action)\n",
    "                    new_str = str + game.get_action_str(action) + ' --> '\n",
    "                    get_sequence(game, Q, subX, subY, new_str)\n",
    "\n",
    "        get_sequence(game, Q, x, y)\n",
    "\n",
    "\n",
    "class DeepGridLearner2D(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DeepGridLearner2D, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_actions)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            y = layer(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, game, x, y, action):\n",
    "        newX, newY = game.action(x, y, action)\n",
    "        if self.capacity < len(self.memory):\n",
    "            return\n",
    "        self.memory.append([x, y, action, newX, newY])\n",
    "\n",
    "    def sample(self, size):\n",
    "        return np.array(self.memory)[np.random.randint(0, len(self.memory), size)]\n",
    "\n",
    "\n",
    "def generate_coordinates(game):\n",
    "    x, y = np.random.randint(0, game.x), np.random.randint(0, game.y)\n",
    "    while (x, y) in game.terminal_states:\n",
    "        x, y = np.random.randint(0, game.x), np.random.randint(0, game.y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def q1():\n",
    "    game = GridGame2D(grid)\n",
    "    grid_learner = GridLearner2D(DISCOUNT_FACTOR, NEGATIVE_REWARD)\n",
    "    grid_learner.train(game)\n",
    "    for y in range(game.y):\n",
    "        for x in range(game.x):\n",
    "            index = x + game.x * y\n",
    "            max_points = np.ndarray.flatten(np.argwhere(grid_learner.Q[index] == np.max(grid_learner.Q[index])))\n",
    "            action_str = ''\n",
    "            for action in max_points:\n",
    "                if action == 0:\n",
    "                    action_str += '→'\n",
    "                elif action == 1:\n",
    "                    action_str += '←'\n",
    "                elif action == 2:\n",
    "                    action_str += '↑'\n",
    "                elif action == 3:\n",
    "                    action_str += '↓'\n",
    "            print(action_str, end='\\t')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "def q2():\n",
    "    game = GridGame2D(grid)\n",
    "    num_episodes = 1000\n",
    "    gamma = DISCOUNT_FACTOR\n",
    "    r = NEGATIVE_REWARD\n",
    "    sequence_limit = 5\n",
    "    target_update = 10\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    policy_model = DeepGridLearner2D(game.num_actions).to(device)\n",
    "    target_model = DeepGridLearner2D(game.num_actions).to(device)\n",
    "    target_model.eval()\n",
    "    target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.SGD(policy_model.parameters(), lr=0.01)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        s = torch.Tensor([generate_coordinates(game)]).float().to(device)\n",
    "        in_terminal = False\n",
    "        sequence_i = 0\n",
    "\n",
    "        while not in_terminal:\n",
    "            q_0, action = policy_model(s).max(1)\n",
    "            newX, newY = game.action(int(s[0][0].cpu()), int(s[0][1].cpu()), int(action.cpu()))\n",
    "            in_terminal = (newX, newY) in game.terminal_states\n",
    "            reward = torch.Tensor([r]).float().to(device)\n",
    "            expected_value = reward\n",
    "            if not in_terminal:\n",
    "                expected_value += target_model(\n",
    "                    torch.Tensor([[newX, newY]]).float().to(device)\n",
    "                ).max().squeeze().to(device) * gamma\n",
    "\n",
    "            loss = criterion(q_0, expected_value)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in policy_model.parameters():\n",
    "                param.grad.data.clamp(-1, 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if (newX, newY) in game.terminal_states:\n",
    "                in_terminal = True\n",
    "\n",
    "            sequence_i += 1\n",
    "            if sequence_i >= sequence_limit:\n",
    "                x, y = generate_coordinates(game)\n",
    "                s = torch.Tensor(\n",
    "                    [[x, y]]\n",
    "                ).float().to(device)\n",
    "                sequence_i = 0\n",
    "            else:\n",
    "                s = torch.Tensor(\n",
    "                    [[newX, newY]]\n",
    "                ).float().to(device)\n",
    "\n",
    "        if (episode + 1) % target_update == 0:\n",
    "            target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y in range(game.y):\n",
    "            for x in range(game.x):\n",
    "                output = policy_model(torch.from_numpy(np.array([[x, y]])).float().to(device)).cpu().numpy()\n",
    "                max_points = np.ndarray.flatten(np.argwhere(output == np.max(output)))\n",
    "                action_str = ''\n",
    "                for action in max_points:\n",
    "                    if action == 0:\n",
    "                        action_str += '→'\n",
    "                    elif action == 1:\n",
    "                        action_str += '←'\n",
    "                    elif action == 2:\n",
    "                        action_str += '↑'\n",
    "                    elif action == 3:\n",
    "                        action_str += '↓'\n",
    "                print(action_str, end='\\t')\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "def q3():\n",
    "    game = GridGame2D(grid)\n",
    "    num_episodes = 100\n",
    "    gamma = DISCOUNT_FACTOR\n",
    "    r = NEGATIVE_REWARD\n",
    "    sequence_limit = 5\n",
    "    target_update = 10\n",
    "    batch_size = 32\n",
    "    eps = 1e-2\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    replay = ReplayMemory(10000)\n",
    "\n",
    "    policy_model = DeepGridLearner2D(game.num_actions).to(device)\n",
    "    target_model = DeepGridLearner2D(game.num_actions).to(device)\n",
    "    target_model.eval()\n",
    "    target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.SGD(policy_model.parameters(), lr=0.01)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        x, y = generate_coordinates(game)\n",
    "        while (x, y) not in game.terminal_states:\n",
    "            s = torch.Tensor([[x, y]]).float().to(device)\n",
    "            sequence_i = 0\n",
    "            with torch.no_grad():\n",
    "                if np.random.uniform() > eps:\n",
    "                    _, action = policy_model(s).max(1)\n",
    "                else:\n",
    "                    action = torch.Tensor([np.random.randint(game.num_actions)]).to(device)\n",
    "            newX, newY = game.action(int(s[0][0].cpu()), int(s[0][1].cpu()), int(action.cpu()))\n",
    "            replay.push(game, x, y, int(action.cpu()))\n",
    "\n",
    "            batch = replay.sample(batch_size)\n",
    "\n",
    "            states = torch.Tensor(batch[:, 0:2]).float().to(device)\n",
    "            actions = torch.Tensor(batch[:, 2:3]).view(1, batch_size).type(torch.int64).to(device)\n",
    "            new_states = torch.Tensor(batch[:, 3:5]).to(device)\n",
    "\n",
    "            q_0 = policy_model(states).gather(1, actions).squeeze()\n",
    "\n",
    "            expected_value = torch.full((batch_size, ), r, dtype=torch.float64).to(device)\n",
    "            non_terminal_mask = torch.Tensor(\n",
    "                [(new_states[i][0], new_states[i][1]) not in game.terminal_states for i in range(batch_size)]\n",
    "            ).bool().to(device)\n",
    "            expected_value[non_terminal_mask] += target_model(new_states[non_terminal_mask]).max().squeeze() * gamma\n",
    "            loss = criterion(q_0, expected_value)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in policy_model.parameters():\n",
    "                param.grad.data.clamp(-1, 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            sequence_i += 1\n",
    "            if sequence_i >= sequence_limit:\n",
    "                x, y = generate_coordinates(game)\n",
    "            else:\n",
    "                x, y = newX, newY\n",
    "\n",
    "        if (episode + 1) % target_update == 0:\n",
    "            target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y in range(game.y):\n",
    "            for x in range(game.x):\n",
    "                output = target_model(torch.from_numpy(np.array([[x, y]])).float().to(device)).cpu().numpy()\n",
    "                max_points = np.ndarray.flatten(np.argwhere(output == np.max(output)))\n",
    "                action_str = ''\n",
    "                for action in max_points:\n",
    "                    if action == 0:\n",
    "                        action_str += '→'\n",
    "                    elif action == 1:\n",
    "                        action_str += '←'\n",
    "                    elif action == 2:\n",
    "                        action_str += '↑'\n",
    "                    elif action == 3:\n",
    "                        action_str += '↓'\n",
    "                print(action_str, end='\\t')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "←↑\t←\t←\t←\t←↓\t\n",
      "\n",
      "↑\t←↑\t←↑\t→←↑↓\t↓\t\n",
      "\n",
      "↑\t←↑\t→←↑↓\t→↓\t↓\t\n",
      "\n",
      "↑\t→←↑↓\t→↓\t→↓\t↓\t\n",
      "\n",
      "→↑\t→\t→\t→\t→↓\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→↑\t→↑\t→↑\t→↑\t→↑\t\n",
      "\n",
      "→↑\t→↑\t→↑\t→↑\t→↑\t\n",
      "\n",
      "→↑\t→↑\t→↑\t→↑\t→↑\t\n",
      "\n",
      "→↑\t→↑\t→↑\t→↑\t→↑\t\n",
      "\n",
      "→↑\t→↑\t→↑\t→↑\t→↑\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→↑\t→↑\t→↑\t→↑\t→↑\t\n",
      "\n",
      "→↑\t→↑\t→↑\t→↑\t→↑\t\n",
      "\n",
      "→↑\t→↑\t→↑\t→↑\t→↑\t\n",
      "\n",
      "→↑\t→↑\t→↑\t→↑\t→↑\t\n",
      "\n",
      "→↑\t→↑\t→↑\t→↑\t→↑\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q3()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}