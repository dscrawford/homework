{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "trainFile = 'train.xml'\n",
    "testFile = 'test.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Dataset(TensorDataset):\n",
    "    def __init__(self, p=None, h=None, l=None):\n",
    "        if p is None and h is None and l is None :\n",
    "            return\n",
    "        self.p = torch.from_numpy(np.array(p))\n",
    "        self.h = torch.from_numpy(np.array(h))\n",
    "        labels = [[1 if i == 1 else 0, 1 if i ==2 else 0] for i in l]\n",
    "        self.labels = torch.from_numpy(np.array(labels)).type(torch.FloatTensor)\n",
    "    \n",
    "    def to(self, device):\n",
    "        newDataset = Dataset()\n",
    "        newDataset.p = self.p.to(device)\n",
    "        newDataset.h = self.h.to(device)\n",
    "        newDataset.labels = self.labels.to(device)\n",
    "        return newDataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.p)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return [self.p[item], self.h[item]], self.labels[item]\n",
    "    \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_length, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_length, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        n = len(data[0])\n",
    "        et = self.embedding(data[0])\n",
    "        #print(et.shape)\n",
    "        ht = self.embedding(data[1])\n",
    "        rt, _ = self.lstm(et.view(n,1,-1))\n",
    "        rh, _ = self.lstm(ht.view(n,1,-1))\n",
    "        rth = torch.cat((rt.view(n,-1),rh.view(n,-1)), dim=0)\n",
    "        fc = self.fc(torch.sum(rth,dim=0))\n",
    "        return F.softmax(fc, dim=0)\n",
    "    \n",
    "def train(model, optimizer, criterion, data, sampler):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for i in sampler:\n",
    "        optimizer.zero_grad()\n",
    "        batch_data = data[i][0]\n",
    "        batch_label = data[i][1]\n",
    "        predictions = model(batch_data)\n",
    "        loss = criterion(predictions, batch_label)\n",
    "        acc = equal(batch_label, predictions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(sampler), epoch_acc / len(sampler)\n",
    "\n",
    "def evaluate(model, criterion, data, sampler):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in sampler:\n",
    "            batch_data = data[i][0]\n",
    "            batch_label = data[i][1]\n",
    "            predictions = model(batch_data)\n",
    "            loss = criterion(predictions, batch_label)\n",
    "            acc = equal(batch_label, predictions)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(sampler), epoch_acc / len(sampler)\n",
    "\n",
    "def equal(y_true, y_pred):\n",
    "    y_pred = y_pred.data.argmax()\n",
    "    y_true = y_true.data.argmax()\n",
    "    return (y_true == y_pred).float()\n",
    "# Parses XML file for entailment-corpus.\n",
    "def parseXML(trainFile):\n",
    "    tree = ET.parse(trainFile)\n",
    "    root = tree.getroot()\n",
    "    pairs = root.findall('./pair')\n",
    "    p = [re.sub(\"[^\\w]\", \" \", pair.find('./t').text.lower()).split() for pair in pairs]\n",
    "    h = [re.sub(\"[^\\w]\", \" \", pair.find('./h').text.lower()).split() for pair in pairs]\n",
    "    l = [pair.get('value') for pair in pairs] \n",
    "    return p,h,l\n",
    "\n",
    "def createWordToIntegerDict(words):\n",
    "    d = defaultdict(int)\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        if d[word] == 0:\n",
    "            i += 1\n",
    "            d[word] = i\n",
    "    return d\n",
    "\n",
    "def transformWordsToIntegers(w, d):\n",
    "    return [[d[word] for word in words] for words in w]\n",
    "    \n",
    "def transformListsToUniformLength(w, maxLength, padding=0):\n",
    "    L = [[padding for _ in range(maxLength)] for _ in w]\n",
    "    for i,list in enumerate(w):\n",
    "        end = len(list) if len(list) <= maxLength else maxLength\n",
    "        L[i][0:end] = list[0:end]\n",
    "    return L\n",
    "    \n",
    "def prepareDataset(p, h, l, wd, ld, maxLength=0):\n",
    "    if maxLength == 0:\n",
    "        maxLength = max([len(s) for s in p + h])\n",
    "    print(maxLength)\n",
    "    p = transformListsToUniformLength(transformWordsToIntegers(p, wd),maxLength=maxLength)\n",
    "    h = transformListsToUniformLength(transformWordsToIntegers(h, wd),maxLength=maxLength)\n",
    "    l = [ld[l] for l in l]\n",
    "    return Dataset(p,h,l), maxLength\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "62\n",
      "62\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "pTrain, hTrain, lTrain = parseXML(trainFile)\n",
    "pTest, hTest, lTest = parseXML(testFile)\n",
    "wd = createWordToIntegerDict([word for words in pTrain for word in words] + \n",
    "                             [word for words in hTrain for word in words])\n",
    "ld = createWordToIntegerDict([label for label in lTrain])\n",
    "trainData, maxLength = prepareDataset(pTrain, hTrain, lTrain,wd,ld,maxLength=0)\n",
    "testData, _ = prepareDataset(pTest, hTest, lTest,wd,ld,maxLength=maxLength)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNN(len(wd), 10, 100, 2)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "trainData = trainData.to(device)\n",
    "testData = testData.to(device)\n",
    "\n",
    "trainSample, testSample = (RandomSampler(trainData), SequentialSampler(testData))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch: 01 | Epoch Time: 4.078298568725586s\n",
      "\tTrain Loss: 0.756 | Train Acc: 49.03%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 02 | Epoch Time: 4.088341474533081s\n",
      "\tTrain Loss: 0.769 | Train Acc: 46.21%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 03 | Epoch Time: 4.0809080600738525s\n",
      "\tTrain Loss: 0.763 | Train Acc: 47.27%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 04 | Epoch Time: 4.073481559753418s\n",
      "\tTrain Loss: 0.756 | Train Acc: 49.03%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 05 | Epoch Time: 4.078616380691528s\n",
      "\tTrain Loss: 0.751 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.751 |  Val. Acc: 50.00%\n",
      "Epoch: 06 | Epoch Time: 4.1003196239471436s\n",
      "\tTrain Loss: 0.751 | Train Acc: 50.44%\n",
      "\t Val. Loss: 0.751 |  Val. Acc: 50.00%\n",
      "Epoch: 07 | Epoch Time: 4.0715556144714355s\n",
      "\tTrain Loss: 0.755 | Train Acc: 49.21%\n",
      "\t Val. Loss: 0.751 |  Val. Acc: 50.00%\n",
      "Epoch: 08 | Epoch Time: 4.086809396743774s\n",
      "\tTrain Loss: 0.766 | Train Acc: 46.56%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 09 | Epoch Time: 4.083940505981445s\n",
      "\tTrain Loss: 0.751 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.748 |  Val. Acc: 50.00%\n",
      "Epoch: 10 | Epoch Time: 4.070260524749756s\n",
      "\tTrain Loss: 0.762 | Train Acc: 47.27%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 11 | Epoch Time: 4.073397874832153s\n",
      "\tTrain Loss: 0.756 | Train Acc: 49.03%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 12 | Epoch Time: 4.115708827972412s\n",
      "\tTrain Loss: 0.760 | Train Acc: 48.15%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 13 | Epoch Time: 4.123865127563477s\n",
      "\tTrain Loss: 0.752 | Train Acc: 49.91%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 14 | Epoch Time: 4.071859836578369s\n",
      "\tTrain Loss: 0.765 | Train Acc: 47.62%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 15 | Epoch Time: 4.077173471450806s\n",
      "\tTrain Loss: 0.754 | Train Acc: 49.56%\n",
      "\t Val. Loss: 0.747 |  Val. Acc: 50.38%\n",
      "Epoch: 16 | Epoch Time: 4.084821701049805s\n",
      "\tTrain Loss: 0.755 | Train Acc: 49.21%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 17 | Epoch Time: 4.0548107624053955s\n",
      "\tTrain Loss: 0.753 | Train Acc: 49.74%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 18 | Epoch Time: 4.05831503868103s\n",
      "\tTrain Loss: 0.766 | Train Acc: 46.91%\n",
      "\t Val. Loss: 0.751 |  Val. Acc: 50.00%\n",
      "Epoch: 19 | Epoch Time: 4.0813353061676025s\n",
      "\tTrain Loss: 0.753 | Train Acc: 49.91%\n",
      "\t Val. Loss: 0.736 |  Val. Acc: 50.50%\n",
      "Epoch: 20 | Epoch Time: 4.175707101821899s\n",
      "\tTrain Loss: 0.766 | Train Acc: 47.27%\n",
      "\t Val. Loss: 0.751 |  Val. Acc: 50.00%\n",
      "Epoch: 21 | Epoch Time: 4.059087514877319s\n",
      "\tTrain Loss: 0.758 | Train Acc: 48.85%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 22 | Epoch Time: 4.073640584945679s\n",
      "\tTrain Loss: 0.748 | Train Acc: 50.97%\n",
      "\t Val. Loss: 0.749 |  Val. Acc: 50.50%\n",
      "Epoch: 23 | Epoch Time: 4.073150873184204s\n",
      "\tTrain Loss: 0.755 | Train Acc: 49.03%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 24 | Epoch Time: 4.075548887252808s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 25 | Epoch Time: 4.092954635620117s\n",
      "\tTrain Loss: 0.747 | Train Acc: 50.79%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 26 | Epoch Time: 4.059514760971069s\n",
      "\tTrain Loss: 0.743 | Train Acc: 52.20%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 27 | Epoch Time: 4.072988986968994s\n",
      "\tTrain Loss: 0.756 | Train Acc: 49.03%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 28 | Epoch Time: 4.064184188842773s\n",
      "\tTrain Loss: 0.753 | Train Acc: 49.91%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 29 | Epoch Time: 4.114161729812622s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.751 |  Val. Acc: 50.00%\n",
      "Epoch: 30 | Epoch Time: 4.159354209899902s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.746 |  Val. Acc: 50.00%\n",
      "Epoch: 31 | Epoch Time: 4.152942419052124s\n",
      "\tTrain Loss: 0.761 | Train Acc: 48.15%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 32 | Epoch Time: 4.065839052200317s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 33 | Epoch Time: 4.170593500137329s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 34 | Epoch Time: 4.217894077301025s\n",
      "\tTrain Loss: 0.768 | Train Acc: 47.09%\n",
      "\t Val. Loss: 0.749 |  Val. Acc: 50.50%\n",
      "Epoch: 35 | Epoch Time: 4.197803258895874s\n",
      "\tTrain Loss: 0.752 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 36 | Epoch Time: 4.158116102218628s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 37 | Epoch Time: 4.047940254211426s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 38 | Epoch Time: 4.058919668197632s\n",
      "\tTrain Loss: 0.760 | Train Acc: 48.68%\n",
      "\t Val. Loss: 0.749 |  Val. Acc: 50.50%\n",
      "Epoch: 39 | Epoch Time: 4.048205375671387s\n",
      "\tTrain Loss: 0.754 | Train Acc: 48.85%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 40 | Epoch Time: 4.045809507369995s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 41 | Epoch Time: 4.0674214363098145s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 42 | Epoch Time: 4.09258246421814s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 43 | Epoch Time: 4.090015172958374s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 44 | Epoch Time: 4.068487644195557s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 45 | Epoch Time: 4.079974889755249s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 46 | Epoch Time: 4.0635857582092285s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.750 |  Val. Acc: 50.00%\n",
      "Epoch: 47 | Epoch Time: 4.055621147155762s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 48 | Epoch Time: 4.083039999008179s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 49 | Epoch Time: 4.06061053276062s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 50 | Epoch Time: 4.077737808227539s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 51 | Epoch Time: 4.07860803604126s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 52 | Epoch Time: 4.050934553146362s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 53 | Epoch Time: 4.0607874393463135s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.749 |  Val. Acc: 50.00%\n",
      "Epoch: 54 | Epoch Time: 4.0767247676849365s\n",
      "\tTrain Loss: 0.756 | Train Acc: 49.21%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 55 | Epoch Time: 4.053173303604126s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 56 | Epoch Time: 4.06705379486084s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 57 | Epoch Time: 4.0699663162231445s\n",
      "\tTrain Loss: 0.765 | Train Acc: 47.27%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 58 | Epoch Time: 4.102448463439941s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 59 | Epoch Time: 4.073269844055176s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 60 | Epoch Time: 4.062304973602295s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 61 | Epoch Time: 4.074126958847046s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.753 |  Val. Acc: 50.00%\n",
      "Epoch: 62 | Epoch Time: 4.354804039001465s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n",
      "Epoch: 63 | Epoch Time: 4.133505582809448s\n",
      "\tTrain Loss: 0.753 | Train Acc: 50.09%\n",
      "\t Val. Loss: 0.752 |  Val. Acc: 50.00%\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "N_EPOCHS = 200\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    t = time.time()\n",
    "    train_loss, train_acc = train(model, optimizer, criterion, trainData, trainSample)\n",
    "    valid_loss, valid_acc = evaluate(model, criterion, testData, testSample)\n",
    "    epoch_secs = time.time() - t\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = nn.Conv2d((135,2), stride=(2,1), padding=(4,2), dilation=(3,1))\n",
    "input = torch.randn(135,2)\n",
    "\n",
    "output = m(input)\n",
    "output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}