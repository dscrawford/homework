{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import time, sys\n",
    "import math\n",
    "\n",
    "# Code made by Brain Khuu: https://stackoverflow.com/questions/3160699/python-progress-bar\n",
    "# update_progress() : Displays or updates a console progress bar\n",
    "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
    "## A value under 0 represents a 'halt'.\n",
    "## A value at 1 or bigger represents 100%\n",
    "def update_progress(progress):\n",
    "    \n",
    "    barLength = 10 # Modify this to change the length of the progress bar\n",
    "    status = \"\"\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "        status = \"error: progress var must be float\\r\\n\"\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "        status = \"\\nHalt...\\r\\n\"\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "        status = \"\\nDone...\\r\\n\"\n",
    "    block = int(round(barLength*progress))\n",
    "    text = \"\\rPercent: [{0}] {1}% {2}\".format( \"#\"*block + \"-\"*(barLength-block), progress*100, status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Too many arguments\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "folder = \"\"\n",
    "if (len(sys.argv) > 2):\n",
    "    print \"ERROR: Too many arguments\"\n",
    "    sys.exit(1)\n",
    "folder = \"\" if len(sys.argv) == 1 else isys.argv[1]\n",
    "if (folder != \"\" and folder[len(folder) -1] != '/'):\n",
    "    folder += '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense multiple whitespaces into one, grab only alphabetic words, convert to lowercase\n",
    "# and return array of words.\n",
    "def getAllWordsFromString(words):\n",
    "    return re.sub('\\s+', ' ',re.sub('[^a-zA-Z1-9]+', ' ', words)).strip().lower().split(\" \")\n",
    "\n",
    "# getCountOfWords\n",
    "# Create a dictionary with the count of each word in a string.\n",
    "def getCountOfWords(words, allUniqueWords):\n",
    "    allUniqueWordsDict = { i : 0 for i in allUniqueWords }\n",
    "    counts = Counter(getAllWordsFromString(words))\n",
    "    counts = {k : v for k, v in dict(counts).items() if k in allUniqueWordsDict}\n",
    "    return mergeTwoDicts(allUniqueWordsDict, counts)\n",
    "\n",
    "#getBernoulliWords\n",
    "# Create a dictionary that shows the existence of words as 1 or 0.\n",
    "def getBernoulliWords(words, allUniqueWords):\n",
    "    counts = getCountOfWords(words, allUniqueWords)\n",
    "    #Transform counts into existense\n",
    "    return { k : (0 if v == 0 else 1) for k , v in counts.items()}\n",
    "        \n",
    "def getCountOfWordsWithProgressBar(words, allUniqueWords, progress):\n",
    "    progress = round(progress,3)\n",
    "    update_progress(progress)\n",
    "    return getCountOfWords(words, allUniqueWords)\n",
    "\n",
    "def getBernoulliWithProgressBar(words, allUniqueWords, progress):\n",
    "    progress = round(progress,3)\n",
    "    update_progress(progress)\n",
    "    return getBernoulliWords(words, allUniqueWords)\n",
    "\n",
    "def mergeTwoDicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getProduct of Probabilities\n",
    "# returns the log probability sum of all of the elements based on bayes\n",
    "# works with both bernoulli and bag of words model\n",
    "def getProductOfProbabities(text, T):\n",
    "    featureSums = T.sum().loc[[w for w in getAllWordsFromString(text) if w in T.columns]]\n",
    "    totalWords  = T.sum().sum()\n",
    "    return np.log( (featureSums + 1) / (totalWords + len(T.columns))).sum()\n",
    "\n",
    "def naiveBayesOnModel(text, T):\n",
    "    p_0 = np.log(len(T[T['isSpam'] == 0]) / len(T)) + getProductOfProbabities(text, (T[T['isSpam'] == 0]).drop('isSpam', axis=1))\n",
    "    p_1 = np.log(len(T[T['isSpam'] == 1]) / len(T)) + getProductOfProbabities(text, (T[T['isSpam'] == 1]).drop('isSpam', axis=1))\n",
    "    return 0 if p_0 > p_1 else 1\n",
    "    \n",
    "def getDirectoryContents(dataDirectory):\n",
    "    contents = np.array([])\n",
    "    for fileName in os.listdir(dataDirectory):\n",
    "        contents = np.append(contents, [open(dataDirectory + fileName).read()])\n",
    "    return contents\n",
    "\n",
    "def getBagOfWordsDataFrame(data, allUniqueWords):\n",
    "    print \"Creating DataFrame with Bag Of Words as the feature...\"\n",
    "    attributes = set(allUniqueWords)\n",
    "    df = pd.DataFrame([getCountOfWordsWithProgressBar(d[1], attributes, i / (len(data) - 1))\n",
    "                       for i,d in enumerate(data)])\n",
    "    df.insert(0, 'isSpam', [d[0] for d in data])\n",
    "    return df\n",
    "\n",
    "def getBernoulliDataFrame(data, allUniqueWords):\n",
    "    print \"Creating DataFrame with Bernoulli model as the feature...\"\n",
    "    attributes = set(allUniqueWords)\n",
    "    df = pd.DataFrame([getBernoulliWithProgressBar(d[1], attributes, i / (len(data) - 1))\n",
    "                       for i,d in enumerate(data)])\n",
    "    df.insert(0, 'isSpam', [d[0] for d in data])\n",
    "    return df\n",
    "\n",
    "def getNaiveBayesPredictions(Test, Train):\n",
    "    return Test.apply(lambda x: naiveBayesOnModel(x['text'], Train), axis=1)\n",
    "\n",
    "def getAccuracyOnNaiveBayes(Test, Train):\n",
    "    return sum(Test.apply(lambda x: naiveBayesOnModel(x['text'], Train) == x['isSpam'], axis=1)) / len(Test)    \n",
    "\n",
    "def PredictWithLR(T, W):\n",
    "    bias = W[0]\n",
    "    PY_1 = 1 / (1 + math.exp(bias + \n",
    "                             T.apply(lambda x: (T[x].sum() / T.sum().sum()) * W[x]).sum()))\n",
    "    PY_0 = 1 - PY_1\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbYIsZero(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "def getProbYIsOne(scores):\n",
    "    return 1 - getProbYIsZero(scores)\n",
    "\n",
    "def getWeight(W, T):\n",
    "    predictions = getPredictions(W, T)\n",
    "    target      = T['isSpam']\n",
    "    attributes  = T.drop('isSpam', axis=1)\n",
    "    attributes.insert(0, 'x_0', 1)\n",
    "    gradient    = np.dot(attributes.T, target - predictions)\n",
    "    return gradient.astype(np.float64)\n",
    "\n",
    "def getLogLikelihood(W, T):\n",
    "    target = T['isSpam']\n",
    "    features = T.drop('isSpam', axis=1)\n",
    "    features.insert(0, 'isSpam', 1)\n",
    "    scores = np.dot(features, W)\n",
    "    return np.sum(target * scores - np.log(1 + np.exp(-scores)))\n",
    "\n",
    "def getPredictions(W, T):\n",
    "    features = T.drop('isSpam', axis=1)\n",
    "    features.insert(0, 'w_0', 1)\n",
    "    return getProbYIsZero(np.dot(T,W)).astype(np.float64)\n",
    "    \n",
    "def getAccuracyOnLR(W, T):\n",
    "    return np.sum([T['isSpam'][i] == prediction.round() for i, prediction in enumerate(getPredictions(W,T))]) / len(T)\n",
    "\n",
    "def splitDataFrame(D, frac):\n",
    "    return (D[0: int(math.floor(len(D) * frac))], D[int(math.floor(len(D) * frac)): len(D)])\n",
    "\n",
    "def L2Regularization(W, V, penalty):\n",
    "    target = V['isSpam']\n",
    "    predictions = getPredictions(W, V)\n",
    "    features = V.drop('isSpam', axis=1)\n",
    "    features.insert(0,'x_0', 1)\n",
    "    gradient = np.dot(features.T, target - predictions)\n",
    "    return (gradient - penalty * W)\n",
    "    \n",
    "def logisticRegression(D, numSteps, learningRate, penalty):\n",
    "    W = np.zeros(len(D.columns))\n",
    "    ham1, ham2   = splitDataFrame(D[D['isSpam'] == 0], 0.7)\n",
    "    spam1, spam2 = splitDataFrame(D[D['isSpam'] == 1], 0.7)\n",
    "    T = ham1.append(spam1).reset_index(drop=True)\n",
    "    V = ham2.append(spam2).reset_index(drop=True)\n",
    "    \n",
    "    print \"Performing gradient descent with weights starting at 0\"\n",
    "    for i in range(1, numSteps):\n",
    "        W += learningRate * getWeight(W,T)\n",
    "        update_progress( round(i / (numSteps - 1), 3))\n",
    "    print \"Regularizing weights using L2 Regularization\"\n",
    "    for i in range(1, numSteps):\n",
    "        W += learningRate * L2Regularization(W, V, penalty)\n",
    "        update_progress( round(i / (numSteps - 1), 3))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHamData  = [[0,f] for f in getDirectoryContents(directory + \"train/ham/\")]\n",
    "trainSpamData = [[1,f] for f in getDirectoryContents(directory + \"train/spam/\")]\n",
    "allTrainData  = trainHamData + trainSpamData\n",
    "testHamData   = [[0,f] for f in getDirectoryContents(directory + \"test/ham/\")]\n",
    "testSpamData  = [[1,f] for f in getDirectoryContents(directory + \"test/spam/\")]\n",
    "allTestData   = pd.DataFrame(testHamData + testSpamData).rename(columns={0: 'isSpam', 1: 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform all files into a single string.\n",
    "allTrainWords = ''.join([f[1] for f in allTrainData])\n",
    "#Retrieve all unique WORDS - Remove all words with numbers/punctuation and replace with space.\n",
    "allUniqueWords = np.unique(getAllWordsFromString(allTrainWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrame with Bernoulli model as the feature...\n",
      "Percent: [##########] 100%  \n",
      "Done...\n",
      "Creating DataFrame with Bernoulli model as the feature...\n",
      "Percent: [##########] 100%  \n",
      "Done...\n",
      "Creating DataFrame with Bag Of Words as the feature...\n",
      "Percent: [##########] 100%  \n",
      "Done...\n",
      "Creating DataFrame with Bag Of Words as the feature...\n",
      "Percent: [##########] 100%  \n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "#Get a dataframe with bernoulli as the feature\n",
    "trainB = getBernoulliDataFrame(allTrainData, allUniqueWords)\n",
    "testB  = getBernoulliDataFrame(testHamData + testSpamData, allUniqueWords)\n",
    "#Get a dataframe with bag of words as a feature for training\n",
    "trainBOW = getBagOfWordsDataFrame(allTrainData, allUniqueWords)\n",
    "testBOW  = getBagOfWordsDataFrame(testHamData + testSpamData, allUniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(Y = 0 | X)\n",
    "def getProbYIsZero(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "# P(Y = 1 | X)\n",
    "def getProbYIsOne(scores):\n",
    "    return 1 - getProbYIsZero(scores)\n",
    "\n",
    "# Returns weight after gradient descent without learning rate.\n",
    "def getWeight(W, T):\n",
    "    predictions = getPredictions(W, T)\n",
    "    target      = T['isSpam']\n",
    "    attributes  = T.drop('isSpam', axis=1)\n",
    "    attributes.insert(0, 'x_0', 1)\n",
    "    gradient    = np.dot(attributes.T, target - predictions)\n",
    "    return gradient.astype(np.float64)\n",
    "\n",
    "# Return log likelihood on dataset\n",
    "def getLogLikelihood(W, T):\n",
    "    target = T['isSpam']\n",
    "    features = T.drop('isSpam', axis=1)\n",
    "    features.insert(0, 'isSpam', 1)\n",
    "    scores = np.dot(features, W)\n",
    "    return np.sum(target * scores - np.log(1 + np.exp(-scores)))\n",
    "\n",
    "# Return all predictions before threshold\n",
    "def getLRPredictions(W, T):\n",
    "    features = T.drop('isSpam', axis=1)\n",
    "    features.insert(0, 'w_0', 1)\n",
    "    return getProbYIsZero(np.dot(T,W)).astype(np.float64)\n",
    "    \n",
    "# Returns \n",
    "def getAccuracyOnLR(W, T):\n",
    "    return np.sum([T['isSpam'][i] == prediction.round() for i, prediction in enumerate(getPredictions(W,T))]) / len(T)\n",
    "\n",
    "def splitDataFrame(D, frac):\n",
    "    return (D[0: int(math.floor(len(D) * frac))], D[int(math.floor(len(D) * frac)): len(D)])\n",
    "\n",
    "def L2Regularization(W, V, penalty):\n",
    "    target = V['isSpam']\n",
    "    predictions = getPredictions(W, V)\n",
    "    features = V.drop('isSpam', axis=1)\n",
    "    features.insert(0,'x_0', 1)\n",
    "    gradient = np.dot(features.T, target - predictions)\n",
    "    return (gradient - penalty * W)\n",
    "    \n",
    "def logisticRegression(D, numSteps, learningRate, penalty):\n",
    "    W = np.zeros(len(D.columns))\n",
    "    ham1, ham2   = splitDataFrame(D[D['isSpam'] == 0], 0.7)\n",
    "    spam1, spam2 = splitDataFrame(D[D['isSpam'] == 1], 0.7)\n",
    "    T = ham1.append(spam1).reset_index(drop=True)\n",
    "    V = ham2.append(spam2).reset_index(drop=True)\n",
    "    \n",
    "    print \"Performing gradient descent with weights starting at 0\"\n",
    "    for i in range(1, numSteps):\n",
    "        W += learningRate * getWeight(W,T)\n",
    "        update_progress( round(i / (numSteps - 1), 3))\n",
    "    print \"Regularizing weights using L2 Regularization\"\n",
    "    for i in range(1, numSteps):\n",
    "        W += learningRate * L2Regularization(W, V, penalty)\n",
    "        update_progress( round(i / (numSteps - 1), 3))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SGDClassifier on Bag of Words features and finding best parameters using GridSearchCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/sklearn/model_selection/_search.py:842: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting SGDClassifier on Bernoulli features and finding best parameters using GridSearchCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/sklearn/model_selection/_search.py:842: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'penalty': ['l1', 'l2', 'elasticnet'], 'loss': ['squared_loss', 'huber', 'epsilon_insensitive', 'hinge', 'log', 'modified_huber', 'squared_hinge']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "parameters = dict(penalty=['l1','l2', 'elasticnet'], \n",
    "                  loss=['squared_loss', 'huber', 'epsilon_insensitive', 'hinge', 'log',\n",
    "                        'modified_huber', 'squared_hinge'])\n",
    "\n",
    "print \"Fitting SGDClassifier on Bag of Words features and finding best parameters using GridSearchCV...\"\n",
    "X = trainBOW.drop('isSpam', axis=1)\n",
    "Y = trainBOW['isSpam']\n",
    "clfBOW = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "clfBOW = GridSearchCV(clfBOW, parameters, cv=5, verbose=0)\n",
    "clfBOW.fit(X,Y)\n",
    "\n",
    "print \"Fitting SGDClassifier on Bernoulli features and finding best parameters using GridSearchCV...\"\n",
    "X = trainB.drop('isSpam', axis=1)\n",
    "Y = trainB['isSpam']\n",
    "clfBer = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "clfBer = GridSearchCV(clfBer, parameters, verbose=0)\n",
    "clfBer.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccPrecRecallF1(Test, predictions):\n",
    "    positives = list(testBOW[testBOW['isSpam'] == 1]['isSpam'].index)\n",
    "    negatives = list(testBOW[testBOW['isSpam'] == 0]['isSpam'].index)\n",
    "    true_positives  = sum(predictions[positives] == testBOW['isSpam'][positives])\n",
    "    false_positives = sum(predictions[positives] != testBOW['isSpam'][positives])\n",
    "    true_negatives  = sum(predictions[negatives] == testBOW['isSpam'][negatives])\n",
    "    false_negatives = sum(predictions[negatives] != testBOW['isSpam'][negatives])\n",
    "    \n",
    "    accuracy        = sum(predictions == testBOW['isSpam']) / len(predictions)\n",
    "    precision       = true_positives / (true_positives + false_positives)\n",
    "    recall          = true_positives / (true_positives + false_negatives)\n",
    "    F1              = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return (accuracy, precision, recall, F1)\n",
    "\n",
    "def printAccPrecRecallF1(Test, predictions, algorithmName, featureType):\n",
    "    acc, prec, recall, F1 = getAccPrecRecallF1(Test, predictions)\n",
    "    print \"Performance with \" + algorithmName + \" using \" + featureType + \" as features:\"\n",
    "    print \"Accuracy : \" + str(acc)\n",
    "    print \"Precision: \" + str(prec)\n",
    "    print \"Recall   : \" + str(recall)\n",
    "    print \"F1       : \" + str(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding weights for Bag of words with Logistic Regression\n",
      "Performing gradient descent with weights starting at 0\n",
      "Percent: [##########] 100%  \n",
      "Done...\n",
      "Regularizing weights using L2 Regularization\n",
      "Percent: [##########] 100%  \n",
      "Done...\n",
      "Finding weights for Bernoulli with Logistic Regression\n",
      "Performing gradient descent with weights starting at 0\n",
      "Percent: [##########] 100%  \n",
      "Done...\n",
      "Regularizing weights using L2 Regularization\n",
      "Percent: [##########] 100%  \n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "print \"Finding weights for Bag of words with Logistic Regression...\"\n",
    "WBOW = logisticRegression(trainBOW, 1000, 0.001, 0.1)\n",
    "print \"Finding weights for Bernoulli with Logistic Regression...\"\n",
    "WBer = logisticRegression(trainB,   1000, 0.001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding predictions for all models\n"
     ]
    }
   ],
   "source": [
    "print \"Finding predictions for all models...\"\n",
    "multipredictions    = getNaiveBayesPredictions(Test=allTestData, Train=trainBOW)\n",
    "discretepredictions = getNaiveBayesPredictions(Test=allTestData, Train=trainB)\n",
    "LRBOWpredictions    = getLRPredictions(WBOW, testBOW).round()\n",
    "LRBerpredictions    = getLRPredictions(WBer, testB).round()\n",
    "SGDBOWpredictions   = clfBOW.predict(testBOW.drop('isSpam', axis=1))\n",
    "SGDBerpredictions   = clfBer.predict(testB.drop('isSpam', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with Multinomial Bayes using Bag of Words as features:\n",
      "Accuracy : 0.948434622468\n",
      "Precision: 0.976982097187\n",
      "Recall   : 0.952618453865\n",
      "F1       : 0.964646464646\n",
      "Performance with Discrete Bayes using Bernoulli as features:\n",
      "Accuracy : 0.948434622468\n",
      "Precision: 1.0\n",
      "Recall   : 0.933174224344\n",
      "F1       : 0.965432098765\n",
      "Performance with Logistic Regression using Bag of Words as features:\n",
      "Accuracy : 0.981583793738\n",
      "Precision: 1.0\n",
      "Recall   : 0.97506234414\n",
      "F1       : 0.987373737374\n",
      "Performance with Logistic Regression using Bernoulli as features:\n",
      "Accuracy : 0.981583793738\n",
      "Precision: 1.0\n",
      "Recall   : 0.97506234414\n",
      "F1       : 0.987373737374\n",
      "Performance with SGDClassifier using Bag of Words as features:\n",
      "Accuracy : 0.944751381215\n",
      "Precision: 0.969309462916\n",
      "Recall   : 0.954659949622\n",
      "F1       : 0.96192893401\n",
      "Performance with SGDClassifier using Bernoulli as features:\n",
      "Accuracy : 0.961325966851\n",
      "Precision: 0.994884910486\n",
      "Recall   : 0.953431372549\n",
      "F1       : 0.973717146433\n"
     ]
    }
   ],
   "source": [
    "target = trainBOW\n",
    "printAccPrecRecallF1(target, multipredictions, \"Multinomial Bayes\", \"Bag of Words\")\n",
    "printAccPrecRecallF1(target, discretepredictions, \"Discrete Bayes\", \"Bernoulli\")\n",
    "printAccPrecRecallF1(target, LRBOWpredictions, \"Logistic Regression\", \"Bag of Words\")\n",
    "printAccPrecRecallF1(target, LRBerpredictions, \"Logistic Regression\", \"Bernoulli\")\n",
    "printAccPrecRecallF1(target, SGDBOWpredictions, \"SGDClassifier\", \"Bag of Words\")\n",
    "printAccPrecRecallF1(target, SGDBerpredictions, \"SGDClassifier\", \"Bernoulli\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
