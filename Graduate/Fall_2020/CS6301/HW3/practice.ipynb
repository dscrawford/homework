{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-f6d34741ccd9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m             \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 55\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[0;31m# %matplotlib inline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################\n",
    "#\n",
    "# xNNs_Code_021_CIFAR.py\n",
    "#\n",
    "# DESCRIPTION\n",
    "#\n",
    "#    CIFAR image classification using PyTorch\n",
    "#\n",
    "# INSTRUCTIONS\n",
    "#\n",
    "#    1. Go to Google Colaboratory: https://colab.research.google.com/notebooks/welcome.ipynb\n",
    "#    2. File - New Python 3 notebook\n",
    "#    3. Cut and paste this file into the cell (feel free to divide into multiple cells)\n",
    "#    4. Runtime - Change runtime type - Hardware accelerator - GPU\n",
    "#    5. Runtime - Run all\n",
    "#\n",
    "# NOTES\n",
    "#\n",
    "#    1. This configuration achieves 90.5% accuracy in 30 epochs with each epoch\n",
    "#       taking ~ 25s on Google Colab.  Accuracy can be improved via\n",
    "#       - Improved training data augmentation\n",
    "#       - Improved network design\n",
    "#       - Improved network training\n",
    "#\n",
    "#    2. Examples (currently commented out) are included for the following\n",
    "#       - Computing the dataset mean and std dev\n",
    "#       - Checkpointing during training and restarting training after a crash\n",
    "#\n",
    "# TO DO\n",
    "#\n",
    "#    0. Quick test to validate performance after refactoring\n",
    "#\n",
    "#    1. Update class name display so names do not need to be manually entered\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# IMPORT\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# torch utils\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# additional libraries\n",
    "import math\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# version check\n",
    "# print(torch.__version__)\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# PARAMETERS\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "# data (general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_DIR = './data'\n",
    "DATA_NUM_CHANNELS = 3\n",
    "DATA_NUM_CLASSES = 10\n",
    "DATA_CROP_ROWS = 28\n",
    "DATA_CROP_COLS = 28\n",
    "\n",
    "# data (for [-1, 1] or 0 mean 1 var normalization of CIFAR-10)\n",
    "DATA_MEAN = (0.5, 0.5, 0.5)\n",
    "DATA_STD_DEV = (0.5, 0.5, 0.5)\n",
    "# DATA_MEAN    = (0.49137914, 0.48213690, 0.44650456)\n",
    "# DATA_STD_DEV = (0.24703294, 0.24348527, 0.26158544)\n",
    "\n",
    "# data (loader)\n",
    "DATA_BATCH_SIZE = 32\n",
    "DATA_NUM_WORKERS = 4\n",
    "\n",
    "# data (for display)\n",
    "DATA_CLASS_NAMES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# model\n",
    "MODEL_LEVEL_0_BLOCKS = 3\n",
    "MODEL_LEVEL_1_BLOCKS = 3\n",
    "MODEL_LEVEL_2_BLOCKS = 3\n",
    "MODEL_LEVEL_0_CHANNELS = 32\n",
    "MODEL_LEVEL_1_CHANNELS = 64\n",
    "MODEL_LEVEL_2_CHANNELS = 128\n",
    "\n",
    "# training (linear warm up with cosine decay learning rate)\n",
    "TRAINING_LR_MAX = 0.001\n",
    "TRAINING_LR_INIT_SCALE = 0.01\n",
    "TRAINING_LR_INIT_EPOCHS = 5\n",
    "TRAINING_LR_FINAL_SCALE = 0.01\n",
    "TRAINING_LR_FINAL_EPOCHS = 25\n",
    "# TRAINING_LR_FINAL_EPOCHS = 2 # uncomment for a quick test\n",
    "TRAINING_NUM_EPOCHS = TRAINING_LR_INIT_EPOCHS + TRAINING_LR_FINAL_EPOCHS\n",
    "TRAINING_LR_INIT = TRAINING_LR_MAX * TRAINING_LR_INIT_SCALE\n",
    "TRAINING_LR_FINAL = TRAINING_LR_MAX * TRAINING_LR_FINAL_SCALE\n",
    "\n",
    "# file\n",
    "FILE_NAME = 'CifarCnn.pt'\n",
    "FILE_SAVE = 0\n",
    "FILE_LOAD = 0\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# DATA\n",
    "#\n",
    "################################################################################\n",
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "\n",
    "# transforms for training and testing datasets\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomCrop((DATA_CROP_ROWS, DATA_CROP_COLS)), transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
    "transform_train2 = transforms.Compose(\n",
    "    [transforms.RandomCrop((DATA_CROP_ROWS, DATA_CROP_COLS)), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
    "transform_train3 = transforms.Compose(\n",
    "    [transforms.RandomCrop((DATA_CROP_ROWS, DATA_CROP_COLS)), transforms.Grayscale(), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
    "transform_test = transforms.Compose([transforms.CenterCrop((DATA_CROP_ROWS, DATA_CROP_COLS)), transforms.ToTensor(),\n",
    "                                     transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
    "\n",
    "\n",
    "\n",
    "# training and testing datasets with applied transform\n",
    "dataset_train = [torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=transform_train),\n",
    "                 torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=False, transform=transform_train2),\n",
    "                 torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=False, transform=transform_train3)]\n",
    "\n",
    "\n",
    "dataset_test = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transform_test)\n",
    "\n",
    "# training and testing datasets loader\n",
    "dataloader_train = torch.utils.data.DataLoader(ConcatDataset(*dataset_train), batch_size=DATA_BATCH_SIZE, shuffle=True,\n",
    "                                               num_workers=DATA_NUM_WORKERS, drop_last=True)\n",
    "print(dataloader_train)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=DATA_BATCH_SIZE, shuffle=False,\n",
    "                                              num_workers=DATA_NUM_WORKERS, drop_last=True)\n",
    "\n",
    "\n",
    "# debug - datasets\n",
    "# print(dataset_train) # displays dataset info\n",
    "# print(dataset_test)  # displays dataset info\n",
    "# data_iterator_train = iter(dataloader_train)\n",
    "# inputs, labels      = data_iterator_train.next()\n",
    "# print(inputs.size())\n",
    "# print(labels.size())\n",
    "# data_iterator_test = iter(dataloader_test)\n",
    "# inputs, labels     = data_iterator_test.next()\n",
    "# print(inputs.size())\n",
    "# print(labels.size())\n",
    "\n",
    "# debug - stats computation\n",
    "# torch.set_printoptions(precision=8)\n",
    "# transform_stats     = transforms.Compose([transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
    "# dataset_stats       = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=transform_stats)\n",
    "# dataloader_stats    = torch.utils.data.DataLoader(dataset_stats, batch_size=DATA_BATCH_SIZE, shuffle=False, num_workers=DATA_NUM_WORKERS, drop_last=True)\n",
    "# num_batches         = 0\n",
    "# data_mean           = torch.tensor([0.0, 0.0, 0.0])\n",
    "# for data in dataloader_stats:\n",
    "#     inputs, labels = data\n",
    "#     data_mean      = data_mean + torch.mean(inputs, (0, 2, 3))\n",
    "#     num_batches    = num_batches + 1\n",
    "# data_mean = data_mean/(1.0*num_batches)\n",
    "# data_mean = data_mean.reshape(1, 3, 1, 1)\n",
    "# print(data_mean)\n",
    "# num_batches = 0\n",
    "# data_std    = torch.tensor([0.0, 0.0, 0.0])\n",
    "# for data in dataloader_stats:\n",
    "#     inputs, labels = data\n",
    "#     data_std       = data_std + torch.mean((inputs - data_mean)*(inputs - data_mean), (0, 2, 3))\n",
    "#     num_batches    = num_batches + 1\n",
    "# data_std = data_std/(1.0*num_batches)\n",
    "# data_std = torch.sqrt(data_std)\n",
    "# data_std = data_std.reshape(1, 3, 1, 1)\n",
    "# print(data_std)\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# NETWORK\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "# define\n",
    "class Model(nn.Module):\n",
    "\n",
    "    # initialization\n",
    "    def __init__(self, data_num_channels, data_num_classes, model_level_0_blocks, model_level_1_blocks,\n",
    "                 model_level_2_blocks, model_level_0_channels, model_level_1_channels, model_level_2_channels):\n",
    "\n",
    "        # parent initialization\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # encoder level 0\n",
    "        self.encoder0 = nn.ModuleList()\n",
    "        self.encoder0.append(\n",
    "            nn.Conv2d(data_num_channels, model_level_0_channels, (3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1),\n",
    "                      groups=1, bias=False, padding_mode='zeros'))\n",
    "        self.encoder0.append(\n",
    "            nn.BatchNorm2d(model_level_0_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.encoder0.append(nn.ReLU())\n",
    "        for n in range(model_level_0_blocks - 1):\n",
    "            self.encoder0.append(\n",
    "                nn.Conv2d(model_level_0_channels, model_level_0_channels, (3, 3), stride=(1, 1), padding=(1, 1),\n",
    "                          dilation=(1, 1), groups=1, bias=False, padding_mode='zeros'))\n",
    "            self.encoder0.append(\n",
    "                nn.BatchNorm2d(model_level_0_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "            self.encoder0.append(nn.ReLU())\n",
    "\n",
    "        # encoder level 1\n",
    "        self.encoder1 = nn.ModuleList()\n",
    "        self.encoder1.append(\n",
    "            nn.MaxPool2d((3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), return_indices=False, ceil_mode=False))\n",
    "        self.encoder1.append(\n",
    "            nn.Conv2d(model_level_0_channels, model_level_1_channels, (3, 3), stride=(1, 1), padding=(1, 1),\n",
    "                      dilation=(1, 1), groups=1, bias=False, padding_mode='zeros'))\n",
    "        self.encoder1.append(\n",
    "            nn.BatchNorm2d(model_level_1_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.encoder1.append(nn.ReLU())\n",
    "        for n in range(model_level_1_blocks - 1):\n",
    "            self.encoder1.append(\n",
    "                nn.Conv2d(model_level_1_channels, model_level_1_channels, (3, 3), stride=(1, 1), padding=(1, 1),\n",
    "                          dilation=(1, 1), groups=1, bias=False, padding_mode='zeros'))\n",
    "            self.encoder1.append(\n",
    "                nn.BatchNorm2d(model_level_1_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "            self.encoder1.append(nn.ReLU())\n",
    "\n",
    "        # encoder level 2\n",
    "        self.encoder2 = nn.ModuleList()\n",
    "        self.encoder2.append(\n",
    "            nn.MaxPool2d((3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), return_indices=False, ceil_mode=False))\n",
    "        self.encoder2.append(\n",
    "            nn.Conv2d(model_level_1_channels, model_level_2_channels, (3, 3), stride=(1, 1), padding=(1, 1),\n",
    "                      dilation=(1, 1), groups=1, bias=False, padding_mode='zeros'))\n",
    "        self.encoder2.append(\n",
    "            nn.BatchNorm2d(model_level_2_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.encoder2.append(nn.ReLU())\n",
    "        for n in range(model_level_2_blocks - 1):\n",
    "            self.encoder2.append(\n",
    "                nn.Conv2d(model_level_2_channels, model_level_2_channels, (3, 3), stride=(1, 1), padding=(1, 1),\n",
    "                          dilation=(1, 1), groups=1, bias=False, padding_mode='zeros'))\n",
    "            self.encoder2.append(\n",
    "                nn.BatchNorm2d(model_level_2_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "            self.encoder2.append(nn.ReLU())\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.decoder.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        self.decoder.append(nn.Flatten())\n",
    "        self.decoder.append(nn.Linear(model_level_2_channels, data_num_classes, bias=True))\n",
    "\n",
    "    # forward path\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encoder level 0\n",
    "        for layer in self.encoder0:\n",
    "            x = layer(x)\n",
    "\n",
    "        # encoder level 1\n",
    "        for layer in self.encoder1:\n",
    "            x = layer(x)\n",
    "\n",
    "        # encoder level 2\n",
    "        for layer in self.encoder2:\n",
    "            x = layer(x)\n",
    "\n",
    "        # decoder\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "\n",
    "        # return\n",
    "        return x\n",
    "\n",
    "\n",
    "# create\n",
    "model = Model(DATA_NUM_CHANNELS, DATA_NUM_CLASSES, MODEL_LEVEL_0_BLOCKS, MODEL_LEVEL_1_BLOCKS, MODEL_LEVEL_2_BLOCKS,\n",
    "              MODEL_LEVEL_0_CHANNELS, MODEL_LEVEL_1_CHANNELS, MODEL_LEVEL_2_CHANNELS)\n",
    "\n",
    "# visualization\n",
    "# print(model)\n",
    "\n",
    "# ONNX export\n",
    "# model_x = torch.randn(1, DATA_NUM_CHANNELS, DATA_CROP_ROWS, DATA_CROP_COLS, dtype=torch.float)\n",
    "# torch.onnx.export(model, model_x, \"CifarCnn.onnx\", verbose=True)\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# TRAIN\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "# start epoch\n",
    "start_epoch = 0\n",
    "\n",
    "\n",
    "# learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    # linear warmup followed by cosine decay\n",
    "    if epoch < TRAINING_LR_INIT_EPOCHS:\n",
    "        lr = (TRAINING_LR_MAX - TRAINING_LR_INIT) * (float(epoch) / TRAINING_LR_INIT_EPOCHS) + TRAINING_LR_INIT\n",
    "    else:\n",
    "        lr = (TRAINING_LR_MAX - TRAINING_LR_FINAL) * max(0.0, math.cos(\n",
    "            ((float(epoch) - TRAINING_LR_INIT_EPOCHS) / (TRAINING_LR_FINAL_EPOCHS - 1.0)) * (\n",
    "                        math.pi / 2.0))) + TRAINING_LR_FINAL\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "# error (softmax cross entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "# specify the device as the GPU if present with fallback to the CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# transfer the network to the device\n",
    "model.to(device)\n",
    "\n",
    "# model loading\n",
    "if FILE_LOAD == 1:\n",
    "    checkpoint = torch.load(FILE_NAME)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "# cycle through the epochs\n",
    "for epoch in range(start_epoch, TRAINING_NUM_EPOCHS):\n",
    "\n",
    "    # initialize train set statistics\n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # set the learning rate for the epoch\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr_schedule(epoch)\n",
    "\n",
    "    # cycle through the train set\n",
    "    for data in dataloader_train:\n",
    "        # extract a batch of data and move it to the appropriate device\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass, loss, backward pass and weight update\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update statistics\n",
    "        training_loss = training_loss + loss.item()\n",
    "        num_batches = num_batches + 1\n",
    "\n",
    "    # initialize test set statistics\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    # no weight update / no gradient needed\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # cycle through the test set\n",
    "        for data in dataloader_test:\n",
    "            # extract a batch of data and move it to the appropriate device\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass and prediction\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # update test set statistics\n",
    "            test_total = test_total + labels.size(0)\n",
    "            test_correct = test_correct + (predicted == labels).sum().item()\n",
    "\n",
    "    # epoch statistics\n",
    "    print('Epoch {0:2d} lr = {1:8.6f} avg loss = {2:8.6f} accuracy = {3:5.2f}'.format(epoch, lr_schedule(epoch), (\n",
    "                training_loss / num_batches) / DATA_BATCH_SIZE, (100.0 * test_correct / test_total)))\n",
    "\n",
    "# model saving\n",
    "# to use this for checkpointing put this code block inside the training loop at the end (e.g., just indent it 4 spaces)\n",
    "# and set 'epoch' to the current epoch instead of TRAINING_NUM_EPOCHS - 1; then if there's a crash it will be possible\n",
    "# to load this checkpoint and restart training from the last complete epoch instead of having to start training at the\n",
    "# beginning\n",
    "if FILE_SAVE == 1:\n",
    "    torch.save({\n",
    "        'epoch': TRAINING_NUM_EPOCHS - 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, FILE_NAME)\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# TEST\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "# initialize test set statistics\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "# initialize class statistics\n",
    "class_correct = list(0. for i in range(DATA_NUM_CLASSES))\n",
    "class_total = list(0. for i in range(DATA_NUM_CLASSES))\n",
    "\n",
    "# no weight update / no gradient needed\n",
    "with torch.no_grad():\n",
    "    # cycle through the test set\n",
    "    for data in dataloader_test:\n",
    "\n",
    "        # extract a batch of data and move it to the appropriate device\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # forward pass and prediction\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # update test set statistics\n",
    "        test_total = test_total + labels.size(0)\n",
    "        test_correct = test_correct + (predicted == labels).sum().item()\n",
    "\n",
    "        # update class statistics\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "# test set statistics\n",
    "print('Accuracy of test set = {0:5.2f}'.format((100.0 * test_correct / test_total)))\n",
    "print('')\n",
    "\n",
    "# class statistics\n",
    "for i in range(DATA_NUM_CLASSES):\n",
    "    print('Accuracy of {0:5s}    = {1:5.2f}'.format(DATA_CLASS_NAMES[i], (100.0 * class_correct[i] / class_total[i])))\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# DISPLAY\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "# set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# extract a batch of data\n",
    "data_iterator = iter(dataloader_test)\n",
    "inputs, labels = data_iterator.next()\n",
    "\n",
    "# images and ground truth labels\n",
    "images = torchvision.utils.make_grid(inputs) / 2 + 0.5\n",
    "np_images = images.numpy()\n",
    "plt.imshow(np.transpose(np_images, (1, 2, 0)))\n",
    "print('Ground truth = ', ' '.join('%5s' % DATA_CLASS_NAMES[labels[j]] for j in range(DATA_BATCH_SIZE)))\n",
    "\n",
    "# move it to the appropriate device\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "# forward pass and prediction\n",
    "outputs = model(inputs)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# predicted labels\n",
    "print('Predicted    = ', ' '.join('%5s' % DATA_CLASS_NAMES[predicted[j]] for j in range(DATA_BATCH_SIZE)))\n",
    "print('')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}