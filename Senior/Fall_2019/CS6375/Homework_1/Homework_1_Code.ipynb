{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import sys\n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lg(P)\n",
    "#  return log_2(P) if P != 0, else return 0\n",
    "def lg(P):\n",
    "    return math.log(P, 2)\n",
    "\n",
    "def Var_Impurity(K, K0, K1):\n",
    "    return (K0 * K1) / K**2\n",
    "\n",
    "def Entropy(K, K0, K1):\n",
    "    if (K0 == 0 or K1 == 0):\n",
    "        return 0\n",
    "    P_0 = K0 / K\n",
    "    P_1 = 1 - P_0\n",
    "    return -P_0 * lg(P_0) - P_1 * lg(P_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Invalid argument for learning algorithm -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "if (len(sys.argv) != 3 and len(sys.argv) != 5):\n",
    "    print('Error: Need either 2 arguments for forests and 5 for decision trees')\n",
    "    sys.exit(1)    \n",
    "fileStr              = ''\n",
    "learner              = None\n",
    "impurityFunction     = None\n",
    "impurityFunctionName = ''\n",
    "pruningAlgorithm     = []\n",
    "\n",
    "dtNames        = ['decisiontree', 'dt']\n",
    "rfNames        = ['randomforest', 'rf']\n",
    "learnerOptions = dtNames + rfNames\n",
    "\n",
    "entropyNames    = ['e', 'entropy']\n",
    "viNames         = ['varianceimpurity', 'vi']\n",
    "impurityOptions = entropyNames + viNames\n",
    "\n",
    "repNames       = ['reducederrorpruning', 'rep', 'reducederror']\n",
    "dbpNames       = ['depthbasedpruning', 'dbp', 'depthbased']\n",
    "naiveNames     = ['naive', 'n']\n",
    "pruningOptions = repNames + dbpNames + naiveNames\n",
    "\n",
    "fileStr      = sys.argv[1].lower()\n",
    "\n",
    "if (sys.argv[2].lower() in learnerOptions):\n",
    "    learner = sys.argv[2].lower()\n",
    "else:\n",
    "    print('Error: Invalid argument for learning algorithm', sys.argv[1])\n",
    "    sys.exit(1)\n",
    "\n",
    "if (learner in rfNames):\n",
    "    if (len(sys.argv) > 3):\n",
    "        print('Error: Too many arguments for random forest')\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    if (len(sys.argv) < 5):\n",
    "        print('Error: Insufficient arguments for decision tree(either need more or less)')\n",
    "        sys.exit(1)\n",
    "\n",
    "    if (sys.argv[3].lower() in impurityOptions):\n",
    "        impurityFunction, impurityFunctionName = (Entropy, 'Entropy') if sys.argv[3].lower() in entropyNames else (Var_Impurity, 'Variance Impurity')\n",
    "    else:\n",
    "        print('Error: Inavlid argument for impurity function(options: \\'entropy\\' or \\'varianceimpurity\\')')\n",
    "        sys.exit(1)\n",
    "    pruningAlgorithm = sys.argv[4].replace(' ', '').lower().split(',')\n",
    "    for option in pruningAlgorithm:\n",
    "        if not option in pruningOptions:\n",
    "            print('Error: Option \\'', option, 'not valid')\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner = 'e'\n",
    "#fileStr = 'c1000_d5000'\n",
    "trainFilename = \"train_\" + fileStr + \".csv\"\n",
    "validFilename = \"valid_\" + fileStr + \".csv\"\n",
    "testFilename = \"test_\"  + fileStr + \".csv\"\n",
    "train = pd.read_csv(\"./all_data/\" + trainFilename)\n",
    "valid = pd.read_csv(\"./all_data/\" + validFilename)\n",
    "test  = pd.read_csv(\"./all_data/\" + testFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = valid.columns = test.columns = list(range(0, len(train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H = Current impurity, X_col = column being tested, S = data set, tA = target attribute, impF = impurity function\n",
    "#Inf_Gain(H, X_col, S, tA, impF)\n",
    "#  sum <- 0\n",
    "#  unique <- unique values in X_col of S\n",
    "#  for x in unique vals\n",
    "#    Sn <- Values in S where X_col = x\n",
    "#    occurences <- number of occurences of each unique value in tA\n",
    "#    sum <- sum + entropy(occurences)\n",
    "#  return H - sum\n",
    "def Inf_Gain(H, X_col, S, tA, impF):\n",
    "    sum = 0\n",
    "    K = len(S)\n",
    "    uniqVals = S[X_col].unique()\n",
    "    for x in uniqVals:\n",
    "        Sn  = S[S[X_col] == x]\n",
    "        occurences = Sn[tA].value_counts()\n",
    "        K0 = occurences[0] if (0 in occurences.index) else 0\n",
    "        K1 = occurences[1] if (1 in occurences.index) else 0\n",
    "        Kv = K0 + K1\n",
    "        sum = sum + (Kv / K) * impF(Kv, K0, K1)\n",
    "    return H - sum;\n",
    "\n",
    "def Get_Best_Attribute(S, tA, cols, impF):\n",
    "    H = impF(len(S), len(S[S[tA] == 0]), len(S[S[tA] == 1]))\n",
    "    maxGain = (0, 0)\n",
    "    for col in cols:\n",
    "        newGain = Inf_Gain(H, col, S[[tA, col]], tA, impF)\n",
    "        maxGain = (col, newGain) if (newGain > maxGain[1])  else maxGain\n",
    "    return maxGain[0]\n",
    "\n",
    "#S = data, tA = target attribute, cols = columns to test on, \n",
    "#impF = impurity function\n",
    "#Grow_Tree(S,tA, cols, impF)\n",
    "#  s_uniq <- All unique classes in target attribute\n",
    "#  if s_uniq has only one value return a leaf with that value\n",
    "#  else\n",
    "#    x_j <- Attribute name with highest gain\n",
    "#    return node(x_j, \n",
    "#                Grow_Tree(S with x_j values == 0, tA, cols, impF),\n",
    "#                Grow_Tree(S with x_j values == 1, tA, cols, impF))\n",
    "def Grow_Tree(S, tA, cols, impF):\n",
    "    s_uniq = S[tA].unique();\n",
    "    if (len(s_uniq) == 1 and s_uniq[0] == 0):\n",
    "        return (0)\n",
    "    elif (len(s_uniq) == 1 and s_uniq[0] == 1):\n",
    "        return (1)\n",
    "    else:\n",
    "        x_j = Get_Best_Attribute(S,tA,cols[cols != tA],impF)\n",
    "        return (x_j,\n",
    "                Grow_Tree(S[S[x_j] == 0], tA, \n",
    "                          cols[cols != x_j], impF), \n",
    "                Grow_Tree(S[S[x_j] == 1], tA, \n",
    "                          cols[cols != x_j], impF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict_Tree()\n",
    "# While not at leaf:\n",
    "#  If value has no nodes on left or right, break\n",
    "#  Find attribute at current node x_j\n",
    "#  Go left if value at attribute x_j == 0, otherwise go right\n",
    "# pred = leaf.val\n",
    "def pred_tree(tree, data):\n",
    "    trav_tree = copy.copy(tree)\n",
    "    atLeaf = False\n",
    "    while True:\n",
    "        if trav_tree == 0 or trav_tree == 1:\n",
    "            break;\n",
    "        if data[trav_tree[0]] == 0:\n",
    "            trav_tree = trav_tree[1]\n",
    "        else:\n",
    "            trav_tree = trav_tree[2]\n",
    "    return trav_tree\n",
    "        \n",
    "def compare_result(tree, data, tA):\n",
    "    pred = pred_tree(tree, data)\n",
    "    return pred == data[tA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to change nodes from splits to leafs\n",
    "#Evaluate starting from nodes holding leaf nodes if they should be replaced\n",
    "#Continue upwards, with next level of nodes being nodes holding the nodes that are holding leafs\n",
    "\n",
    "#V = data set, tA = target attribute\n",
    "#Reduced_Error_Prune_Helper(V,tA, tree)\n",
    "#Start from bottom nodes and continuously replace those nodes with more general nodes.\n",
    "#currTree keeps track of branch needed to be replaced.\n",
    "#Position can help reach a node in the tree\n",
    "#treeAndAcc is a tuple with the tree on the lefthand side, and accuracy and on the right.\n",
    "def Reduced_Error_Prune(V, tA, tree):\n",
    "    treec = copy.copy(tree)\n",
    "    treeAndAcc = (tree, Get_Tree_Accuracy(V, tA, tree))\n",
    "    return Reduced_Error_Prune_Helper(V, tA, treeAndAcc, ())[0]\n",
    "\n",
    "#2^(tree height) = total node estimate in tree.\n",
    "#O(Size of the data * Numbers of nodes in the tree - Number of leafs)\n",
    "def Reduced_Error_Prune_Helper(V, tA, treeAndAcc, position):\n",
    "    currTree = Traverse_Tree(treeAndAcc[0], position)\n",
    "    \n",
    "    if (currTree == 1 or currTree == 0):\n",
    "        return treeAndAcc\n",
    "    if (type(currTree[1]) == tuple):\n",
    "        treeAndAcc = Reduced_Error_Prune_Helper(V, tA, \n",
    "                                                  treeAndAcc, \n",
    "                                                  position + (0,))\n",
    "    if (type(currTree[2]) == tuple):\n",
    "        treeAndAcc = Reduced_Error_Prune_Helper(V, tA, \n",
    "                                                  treeAndAcc, \n",
    "                                                  position + (1,))\n",
    "    \n",
    "    mcl         = Most_Common_Leaf(currTree)\n",
    "    newTree     = Set_Tree(treeAndAcc[0], mcl, copy.copy(position))\n",
    "    newAccuracy = Get_Tree_Accuracy(V, tA, newTree)\n",
    "    \n",
    "    return (newTree, newAccuracy) if newAccuracy >= treeAndAcc[1] else treeAndAcc\n",
    "\n",
    "def Depth_Based_Prune(V, tA, tree):\n",
    "    dMax        = [5,10,15,20,50,100]\n",
    "    bestTree    = copy.copy(tree)\n",
    "    maxAccuracy = Get_Tree_Accuracy(V, tA, tree)\n",
    "    \n",
    "    for depth in dMax:\n",
    "        newTree = Prune_On_Depth(tree, depth, 0)\n",
    "        newAccuracy  = Get_Tree_Accuracy(V, tA, newTree)\n",
    "        (bestTree, maxAccuracy) = (newTree, newAccuracy) if newAccuracy > maxAccuracy else (bestTree, maxAccuracy)\n",
    "    return bestTree\n",
    "\n",
    "def Prune_On_Depth(tree, dMax, depth):\n",
    "    depth += 1\n",
    "    if (tree == 1 or tree == 0):\n",
    "        return tree\n",
    "    if (depth < dMax):\n",
    "        if (type(tree[1]) == tuple):\n",
    "            tree = (tree[0], Prune_On_Depth(tree[1], dMax, depth), tree[2])\n",
    "        if (type(tree[2]) == tuple):\n",
    "            tree = (tree[0], tree[1], Prune_On_Depth(tree[2], dMax, depth))\n",
    "        return tree\n",
    "    else:\n",
    "        return Most_Common_Leaf(tree)\n",
    "    \n",
    "#Traverse and return a node in a tree based on a position array\n",
    "def Traverse_Tree(tree, position):\n",
    "    nTree = copy.copy(tree)\n",
    "    for i in position:\n",
    "        if (i == 0):\n",
    "            nTree = nTree[1]\n",
    "        else:\n",
    "            nTree = nTree[2]\n",
    "    return nTree;\n",
    "\n",
    "#Replace a node in a tree by its position and return the full tree.\n",
    "def Set_Tree(tree, newSubTree, position):\n",
    "    if (len(position) == 0):\n",
    "        return newSubTree\n",
    "    nextPos = position[0]\n",
    "    position = position[1:]\n",
    "    if (nextPos == 0):\n",
    "        return (tree[0], Set_Tree(tree[1],newSubTree,position), tree[2])\n",
    "    if (nextPos == 1):\n",
    "        return (tree[0], tree[1], Set_Tree(tree[2],newSubTree,position))\n",
    "\n",
    "#Returns most common leaf from a tree/subtree.\n",
    "def Most_Common_Leaf(tree):\n",
    "    leafs = Get_All_Leafs(tree)\n",
    "    return max(leafs, key=leafs.count)\n",
    "\n",
    "#Returns a list of all the leafs in a tree\n",
    "def Get_All_Leafs(tree):\n",
    "    if (type(tree) == tuple):\n",
    "        return Get_All_Leafs(tree[1]) + Get_All_Leafs(tree[2])\n",
    "    else:\n",
    "        if (tree == 0):\n",
    "            return [0]\n",
    "        else:\n",
    "            return [1]\n",
    "    \n",
    "#Evaluates accuracy of a tree on target attribute tA based on data D\n",
    "def Get_Tree_Accuracy(D, tA, tree):\n",
    "    results = D.apply(lambda instance: compare_result(tree, instance, tA), axis=1)\n",
    "    return len(results[results == True])/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyze_Prune_Trees(V, T, tA, tree):\n",
    "    for prune in pruningAlgorithm:\n",
    "        if prune in naiveNames:\n",
    "            print(\"Accuracy with naive trees trained on \" + impurityFunctionName +\n",
    "                  \" in \" + testFilename + \":\"\n",
    "                  ,Get_Tree_Accuracy(T, tA, tree))\n",
    "        elif prune in dbpNames:\n",
    "            dbpTree = Depth_Based_Prune(V, tA, t1,tree)\n",
    "            print(\"Accuracy with trees using depth-based pruning trained on \" + impurityFunctionName +\n",
    "                  \" in \" + testFilename + \":\"\n",
    "                  ,Get_Tree_Accuracy(T, tA, dbpTree))\n",
    "        elif prune in repNames:\n",
    "            repTree = Reduced_Error_Prune(V, tA, tree)\n",
    "            print(\"Accuracy with trees using reduced-error pruning trained on \" + impurityFunctionName +\n",
    "                  \"in \" + testFilename + \":\"\n",
    "                  ,Get_Tree_Accuracy(T, tA, repTree))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with random forests in test_c1000_d5000.csv: 0.6342634263426342\n"
     ]
    }
   ],
   "source": [
    "if (learner in dtNames):\n",
    "    print(\"Growing tree on \" + trainFilename)\n",
    "    startTime = datetime.now()\n",
    "    tree = Grow_Tree(train, 1, train.columns, impurityFunction)\n",
    "    print(\"Time taken to grow tree: \", datetime.now() - startTime)\n",
    "    Analyze_Prune_Trees(valid, test, 1, tree)\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    newTrain = train.append(valid)\n",
    "    X = newTrain.drop(columns=[1])\n",
    "    y = newTrain[1]\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X, y)\n",
    "    predict = model.predict(test.drop(columns=[1]))\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    print(\"Accuracy with random forests in \" + testFilename + \":\", accuracy_score(test[1], predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
