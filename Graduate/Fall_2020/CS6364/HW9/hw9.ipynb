{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Made by Daniel Crawford\n",
    "# Student Net ID: dsc160130\n",
    "# Course: CS6364 - Artificial Intelligence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "# hyperparameters\n",
    "DISCOUNT_FACTOR = 0.7\n",
    "NEGATIVE_REWARD = -5\n",
    "LOSS_NEGATIVE_REWARD = -100\n",
    "INITIAL_STATE_PROB = 1 / 25\n",
    "\n",
    "RIGHT = 0\n",
    "LEFT = 1\n",
    "UP = 2\n",
    "DOWN = 3\n",
    "\n",
    "\n",
    "# Cliff Game 2D\n",
    "# 2D grid with 3 different positions\n",
    "# 0 - Empty space\n",
    "# 1 - Target space\n",
    "# 2 - Cliff space\n",
    "class CliffGame2D:\n",
    "    def __init__(self, grid_y, grid_x, start_y, start_x, target_pos_list, cliff_pos_list, base_reward, lose_reward):\n",
    "        # Initialize grid space\n",
    "        self.grid_x = grid_x\n",
    "        self.grid_y = grid_y\n",
    "        self.grid = np.zeros((grid_y, grid_x), dtype=int)\n",
    "        self.start_x, self.start_y = start_x - 1, start_y - 1\n",
    "        self.current_x, self.current_y = self.start_x, self.start_y\n",
    "        for y, x in target_pos_list:\n",
    "            self.grid[y - 1][x - 1] = 1\n",
    "        for y, x in cliff_pos_list:\n",
    "            self.grid[y - 1][x - 1] = 2\n",
    "        self.num_actions = 4\n",
    "        self.base_reward = base_reward\n",
    "        self.lose_reward = lose_reward\n",
    "        self.N = grid_x * grid_y\n",
    "\n",
    "        # Create actions list which can either be up, down, left, or right\n",
    "        self.actions = {\n",
    "            RIGHT: lambda y, x: (y, x + 1) if x + 1 < grid_x else (y, x),\n",
    "            LEFT: lambda y, x: (y, x - 1) if x - 1 >= 0 else (y, x),\n",
    "            UP: lambda y, x: (y - 1, x) if y - 1 > 0 else (y, x),\n",
    "            DOWN: lambda y, x: (y + 1, x) if y + 1 < grid_y else (y, x)\n",
    "        }\n",
    "\n",
    "        self.action_str = {\n",
    "            RIGHT: 'right',\n",
    "            LEFT: 'left',\n",
    "            UP: 'up',\n",
    "            DOWN: 'down'\n",
    "        }\n",
    "\n",
    "    def action(self, action):\n",
    "        # Perform action\n",
    "        self.current_y, self.current_x = self.actions[action](self.current_y, self.current_x)\n",
    "\n",
    "        val = self.grid[self.current_y][self.current_x]\n",
    "\n",
    "        # Compute rewards for new position\n",
    "        done = val == 1\n",
    "\n",
    "        reward = self.base_reward if val != 2 else self.lose_reward\n",
    "\n",
    "        return self.current_y + 1, self.current_x + 1, reward, done\n",
    "\n",
    "    def str(self):\n",
    "        # Create string of current state of board\n",
    "        # 0 - Free space\n",
    "        # 1 - Terminal space\n",
    "        # 2 - Cliff space\n",
    "        # x - player space\n",
    "        # d - player done\n",
    "        # l - player lost\n",
    "        grid_str = ''\n",
    "        for y in range(self.grid_y):\n",
    "            for x in range(self.grid_x):\n",
    "                if (y, x) == (self.current_y, self.current_x):\n",
    "                    letter = 'd' if self.grid[y][x] == 1 else ('l' if self.grid[y][x] == 2 else 0)\n",
    "                    grid_str += letter + '\\t'\n",
    "                else:\n",
    "                    grid_str += str(self.grid[y][x]) + '\\t'\n",
    "            grid_str += '\\n'\n",
    "        return grid_str\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_x, self.current_y = self.start_x, self.start_y\n",
    "\n",
    "    def get_pos(self):\n",
    "        return self.current_y + 1, self.current_x + 1\n",
    "\n",
    "    def get_one_hot_pos(self, y_pos, x_pos):\n",
    "        return np.array([int((y, x) == (y_pos - 1, x_pos - 1)) for y in range(self.grid_y) for x in range(self.grid_x)])\n",
    "\n",
    "    def set_pos(self, y, x):\n",
    "        self.current_x = x\n",
    "        self.current_y = y\n",
    "\n",
    "    def get_action_str(self, action):\n",
    "        return self.action_str[action]\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_size, 8, bias=False),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(8, output_size, bias=False)\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            y = layer(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "game = CliffGame2D(6, 10, 6, 1, [(6, 10)], [(6, i) for i in range(2, 10)], NEGATIVE_REWARD, LOSS_NEGATIVE_REWARD)\n",
    "\n",
    "Transition = namedtuple(\"Transition\", ['y', 'x', 'action', 'reward', 'newY', 'newX', 'done'])\n",
    "\n",
    "def init_weights_0(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # m.weight.data[:] = 0\n",
    "        if m.bias is not None:\n",
    "            m.bias.data[:] = 0\n",
    "\n",
    "def reinforce(game, policy_model, target_model, target_criterion, policy_optimizer, target_optimizer,\n",
    "              device, num_episodes=100, gamma=DISCOUNT_FACTOR, seq_limit=float('inf')):\n",
    "    policy_model.apply(init_weights_0)\n",
    "    target_model.apply(init_weights_0)\n",
    "    input_size = game.N\n",
    "    for episode in range(num_episodes):\n",
    "        game.reset()\n",
    "        # init episode\n",
    "        episode_info = []\n",
    "\n",
    "        y, x = game.get_pos()\n",
    "        policy_model.eval()\n",
    "        target_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for t in count():\n",
    "                state = torch.Tensor(game.get_one_hot_pos(y, x)).view(1, input_size).float().to(device)\n",
    "\n",
    "                output = F.softmax(policy_model(state), dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "                # Choose best action\n",
    "                action = np.random.choice(np.arange(len(output)), p=output)\n",
    "\n",
    "                # Sample next state and check if finished\n",
    "                newY, newX, reward, done = game.action(action)\n",
    "\n",
    "                # Save episode information\n",
    "                episode_info.append(Transition(y, x, action, reward, newY, newX, done))\n",
    "\n",
    "                if done or t > seq_limit:\n",
    "                    break\n",
    "\n",
    "                x, y = newX, newY\n",
    "\n",
    "        episode_reward = sum([t.reward for t in episode_info])\n",
    "        episode_length = len(episode_info)\n",
    "        print('EPISODE ', episode + 1, ' reward: ', episode_reward, ', length: ', episode_length, sep='')\n",
    "        policy_model.train()\n",
    "        target_model.train()\n",
    "\n",
    "        states = [game.get_one_hot_pos(transition.y, transition.x) for transition in episode_info]\n",
    "        actions = [transition.action for transition in episode_info]\n",
    "        reward_returns = [sum(gamma**i * s.reward for i, s in enumerate(episode_info[t:])) for t in range(len(episode_info))]\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).view(len(episode_info), 1).to(device)\n",
    "        reward_returns = torch.FloatTensor(reward_returns).view(len(episode_info), 1).to(device)\n",
    "        baseline = target_model(states)\n",
    "        advantage = reward_returns.clone() - baseline\n",
    "        likelihoods = F.softmax(policy_model(states), dim=1)\n",
    "        selected_likelihoods = likelihoods.gather(1, actions)\n",
    "\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss = -torch.log(selected_likelihoods) * advantage\n",
    "        policy_loss.sum().backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        target_optimizer.zero_grad()\n",
    "        target_loss = target_criterion(baseline, reward_returns)\n",
    "        target_loss.backward()\n",
    "        target_optimizer.step()\n",
    "\n",
    "        # get_sequence(game, policy_model, device)\n",
    "\n",
    "def actor_critic(game, policy_model, target_model, target_criterion, policy_optimizer, target_optimizer,\n",
    "                 trace_decay_theta, trace_decay_w, device, num_episodes=100, gamma=DISCOUNT_FACTOR,\n",
    "                 seq_limit=float('inf')):\n",
    "    policy_model.apply(init_weights_0)\n",
    "    target_model.apply(init_weights_0)\n",
    "    input_size = game.N\n",
    "    for episode in range(num_episodes):\n",
    "        game.reset()\n",
    "        # init episode\n",
    "        episode_info = []\n",
    "\n",
    "        y, x = game.get_pos()\n",
    "        policy_model.train()\n",
    "        target_model.train()\n",
    "\n",
    "        z_theta = torch.Tensor([0 for _ in range(game.num_actions)]).to(device)\n",
    "        z_w = torch.Tensor([0 for _ in range(1)]).to(device)\n",
    "        I = torch.FloatTensor([1]).to(device)\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        for t in count():\n",
    "            state = torch.Tensor(game.get_one_hot_pos(y, x)).view(1, input_size).float().to(device)\n",
    "\n",
    "            output = F.softmax(policy_model(state), dim=1)\n",
    "            action = np.random.choice(np.arange(len(output.squeeze())), p=output.squeeze().detach().cpu().numpy())\n",
    "\n",
    "            newY, newX, reward, done = game.action(action)\n",
    "\n",
    "            new_state = torch.Tensor(game.get_one_hot_pos(newY, newX)).view(1, input_size).float().to(device)\n",
    "\n",
    "            z_w = gamma * trace_decay_w * z_w + target_model(state)\n",
    "\n",
    "            target_optimizer.zero_grad()\n",
    "            z_w.squeeze().backward(retain_graph=True)\n",
    "            target_optimizer.step()\n",
    "\n",
    "            advantage = reward + gamma * target_model(new_state) - target_model(state)\n",
    "\n",
    "            z_theta = gamma * trace_decay_theta * z_theta + I * torch.log(F.softmax(policy_model(state), dim=1))\n",
    "\n",
    "            policy_optimizer.zero_grad()\n",
    "            z_theta.sum().backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            I = gamma * I\n",
    "\n",
    "            z_w = z_w.detach()\n",
    "            z_theta = z_theta.detach()\n",
    "\n",
    "            y, x = newY, newX\n",
    "\n",
    "            episode_reward += reward\n",
    "            episode_length = t\n",
    "\n",
    "            if done or t > seq_limit:\n",
    "                break\n",
    "        print('EPISODE ', episode + 1, ' reward: ', episode_reward, ', length: ', episode_length, sep='')\n",
    "\n",
    "def get_sequence(game, policy_model, device):\n",
    "    with torch.no_grad():\n",
    "        for y in range(game.grid_y):\n",
    "            for x in range(game.grid_x):\n",
    "                state = torch.Tensor(game.get_one_hot_pos(y+1, x+1)).view(1, game.N).float().to(device)\n",
    "                output = policy_model(state).squeeze().cpu().numpy()\n",
    "                actions = np.ndarray.flatten(output == np.max(output))\n",
    "                action_str = ''\n",
    "                if actions[0]:\n",
    "                    action_str += '→'\n",
    "                if actions[1]:\n",
    "                    action_str += '←'\n",
    "                if actions[2]:\n",
    "                    action_str += '↑'\n",
    "                if actions[3]:\n",
    "                    action_str += '↓'\n",
    "                print(action_str, end='\\t')\n",
    "            print()\n",
    "\n",
    "\n",
    "def q1(game):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    policy_model = Model(game.N, game.num_actions).to(device)\n",
    "    target_model = Model(game.N, 1).to(device)\n",
    "    policy_optimizer = torch.optim.Adam(policy_model.parameters(), lr=0.001)\n",
    "    target_optimizer = torch.optim.Adam(target_model.parameters(), lr=0.1)\n",
    "    target_criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    reinforce(game, policy_model, target_model, target_criterion, policy_optimizer, target_optimizer,\n",
    "              device, num_episodes=50, gamma=DISCOUNT_FACTOR)\n",
    "\n",
    "    print('Final Result: ')\n",
    "    get_sequence(game, policy_model, device)\n",
    "\n",
    "def q2(game):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    policy_model = Model(game.N, game.num_actions).to(device)\n",
    "    target_model = Model(game.N, 1).to(device)\n",
    "    policy_optimizer = torch.optim.Adam(policy_model.parameters(), lr=0.001)\n",
    "    target_optimizer = torch.optim.Adam(target_model.parameters(), lr=0.1)\n",
    "    target_criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    actor_critic(game, policy_model, target_model, target_criterion, policy_optimizer, target_optimizer, 0.9, 0.9,\n",
    "              device, num_episodes=10, gamma=DISCOUNT_FACTOR)\n",
    "\n",
    "    print('Final Result: ')\n",
    "    get_sequence(game, policy_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 reward: -3095, length: 87\n",
      "EPISODE 2 reward: -2820, length: 146\n",
      "EPISODE 3 reward: -2670, length: 173\n",
      "EPISODE 4 reward: -2815, length: 107\n",
      "EPISODE 5 reward: -9470, length: 963\n",
      "EPISODE 6 reward: -2315, length: 121\n",
      "EPISODE 7 reward: -5860, length: 507\n",
      "EPISODE 8 reward: -1885, length: 149\n",
      "EPISODE 9 reward: -3405, length: 187\n",
      "EPISODE 10 reward: -7560, length: 315\n",
      "EPISODE 11 reward: -9815, length: 538\n",
      "EPISODE 12 reward: -1905, length: 210\n",
      "EPISODE 13 reward: -6560, length: 381\n",
      "EPISODE 14 reward: -1065, length: 118\n",
      "EPISODE 15 reward: -7500, length: 417\n",
      "EPISODE 16 reward: -13110, length: 570\n",
      "EPISODE 17 reward: -6505, length: 237\n",
      "EPISODE 18 reward: -1100, length: 30\n",
      "EPISODE 19 reward: -1090, length: 180\n",
      "EPISODE 20 reward: -10205, length: 388\n",
      "EPISODE 21 reward: -895, length: 84\n",
      "EPISODE 22 reward: -1425, length: 76\n",
      "EPISODE 23 reward: -4815, length: 260\n",
      "EPISODE 24 reward: -6530, length: 508\n",
      "EPISODE 25 reward: -1740, length: 120\n",
      "EPISODE 26 reward: -6115, length: 140\n",
      "EPISODE 27 reward: -4005, length: 174\n",
      "EPISODE 28 reward: -9630, length: 577\n",
      "EPISODE 29 reward: -9645, length: 618\n",
      "EPISODE 30 reward: -1635, length: 137\n",
      "EPISODE 31 reward: -8940, length: 610\n",
      "EPISODE 32 reward: -1775, length: 32\n",
      "EPISODE 33 reward: -2365, length: 112\n",
      "EPISODE 34 reward: -10990, length: 716\n",
      "EPISODE 35 reward: -1275, length: 122\n",
      "EPISODE 36 reward: -6080, length: 342\n",
      "EPISODE 37 reward: -11680, length: 531\n",
      "EPISODE 38 reward: -5175, length: 294\n",
      "EPISODE 39 reward: -5905, length: 212\n",
      "EPISODE 40 reward: -5925, length: 216\n",
      "EPISODE 41 reward: -5155, length: 328\n",
      "EPISODE 42 reward: -2565, length: 133\n",
      "EPISODE 43 reward: -2230, length: 104\n",
      "EPISODE 44 reward: -7095, length: 336\n",
      "EPISODE 45 reward: -10910, length: 624\n",
      "EPISODE 46 reward: -275, length: 55\n",
      "EPISODE 47 reward: -7875, length: 606\n",
      "EPISODE 48 reward: -8935, length: 400\n",
      "EPISODE 49 reward: -2405, length: 177\n",
      "EPISODE 50 reward: -16360, length: 916\n",
      "Final Result: \n",
      "←\t→\t←\t←\t→\t←\t→\t→\t↑\t←\t\n",
      "→\t→\t↑\t→\t↑\t←\t↑\t←\t→\t←\t\n",
      "←\t←\t→\t→\t←\t←\t←\t↑\t←\t↑\t\n",
      "←\t↑\t↑\t↑\t↑\t↑\t↑\t←\t←\t↑\t\n",
      "↑\t↑\t←\t←\t↑\t↑\t↑\t↑\t←\t↑\t\n",
      "↑\t↑\t↑\t←\t↑\t↑\t←\t←\t←\t↑\t\n"
     ]
    }
   ],
   "source": [
    "q1(game)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE 1 reward: -2515, length: 179\n",
      "EPISODE 2 reward: -6000, length: 116\n",
      "EPISODE 3 reward: -865, length: 115\n",
      "EPISODE 4 reward: -3115, length: 52\n",
      "EPISODE 5 reward: -8320, length: 238\n",
      "EPISODE 6 reward: -5240, length: 192\n",
      "EPISODE 7 reward: -1165, length: 42\n",
      "EPISODE 8 reward: -8410, length: 275\n",
      "EPISODE 9 reward: -2965, length: 117\n",
      "EPISODE 10 reward: -3790, length: 111\n",
      "Final Result: \n",
      "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
      "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
      "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
      "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
      "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
      "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n"
     ]
    }
   ],
   "source": [
    "q2(game)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}