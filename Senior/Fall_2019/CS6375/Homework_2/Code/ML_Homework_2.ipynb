{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import time, sys\n",
    "import math\n",
    "\n",
    "# Code made by Brain Khuu: https://stackoverflow.com/questions/3160699/python-progress-bar\n",
    "# update_progress() : Displays or updates a console progress bar\n",
    "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
    "## A value under 0 represents a 'halt'.\n",
    "## A value at 1 or bigger represents 100%\n",
    "def update_progress(progress):\n",
    "    \n",
    "    barLength = 10 # Modify this to change the length of the progress bar\n",
    "    status = \"\"\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "        status = \"error: progress var must be float\\r\\n\"\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "        status = \"\\nHalt...\\r\\n\"\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "        status = \"\\nDone...\\r\\n\"\n",
    "    block = int(round(barLength*progress))\n",
    "    text = \"\\rPercent: [{0}] {1}% {2}\".format( \"#\"*block + \"-\"*(barLength-block), progress*100, status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/daniel/Documents/homework/Senior/Fall_2019/CS6375/Homework_2/Code/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense multiple whitespaces into one, grab only alphabetic words, convert to lowercase\n",
    "# and return array of words.\n",
    "def getAllWordsFromString(words):\n",
    "    return re.sub('\\s+', ' ',re.sub('[^a-zA-Z1-9]+', ' ', words)).strip().lower().split(\" \")\n",
    "\n",
    "# getCountOfWords\n",
    "# Create a dictionary with the count of each word in a string.\n",
    "def getCountOfWords(words, allUniqueWords):\n",
    "    allUniqueWordsDict = { i : 0 for i in allUniqueWords }\n",
    "    counts = Counter(getAllWordsFromString(words))\n",
    "    counts = {k : v for k, v in dict(counts).items() if k in allUniqueWordsDict}\n",
    "    return mergeTwoDicts(allUniqueWordsDict, counts)\n",
    "\n",
    "#getBernoulliWords\n",
    "# Create a dictionary that shows the existence of words as 1 or 0.\n",
    "def getBernoulliWords(words, allUniqueWords):\n",
    "    counts = getCountOfWords(words, allUniqueWords)\n",
    "    #Transform counts into existense\n",
    "    return { k : (0 if v == 0 else 1) for k , v in counts.items()}\n",
    "        \n",
    "def getCountOfWordsWithProgressBar(words, allUniqueWords, progress):\n",
    "    progress = round(progress,3)\n",
    "    update_progress(progress)\n",
    "    return getCountOfWords(words, allUniqueWords)\n",
    "\n",
    "def getBernoulliWithProgressBar(words, allUniqueWords, progress):\n",
    "    progress = round(progress,3)\n",
    "    update_progress(progress)\n",
    "    return getBernoulliWords(words, allUniqueWords)\n",
    "\n",
    "def mergeTwoDicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getProduct of Probabilities\n",
    "# returns the log probability sum of all of the elements based on bayes\n",
    "# works with both bernoulli and bag of words model\n",
    "def getProductOfProbabities(text, T):\n",
    "    featureSums = T.sum().loc[[w for w in getAllWordsFromString(text) if w in T.columns]]\n",
    "    totalWords  = T.sum().sum()\n",
    "    return np.log( (featureSums + 1) / (totalWords + len(T.columns))).sum()\n",
    "\n",
    "def naiveBayesOnModel(text, T):\n",
    "    p_0 = np.log(len(T[T['isSpam'] == 0]) / len(T)) + getProductOfProbabities(text, (T[T['isSpam'] == 0]).drop('isSpam', axis=1))\n",
    "    p_1 = np.log(len(T[T['isSpam'] == 1]) / len(T)) + getProductOfProbabities(text, (T[T['isSpam'] == 1]).drop('isSpam', axis=1))\n",
    "    return 0 if p_0 > p_1 else 1\n",
    "    \n",
    "def getDirectoryContents(dataDirectory):\n",
    "    contents = np.array([])\n",
    "    for fileName in os.listdir(dataDirectory):\n",
    "        contents = np.append(contents, [open(dataDirectory + fileName).read()])\n",
    "    return contents\n",
    "\n",
    "def getBagOfWordsDataFrame(data, allUniqueWords):\n",
    "    print \"Creating DataFrame with Bag Of Words as the feature...\"\n",
    "    attributes = set(allUniqueWords)\n",
    "    df = pd.DataFrame([getCountOfWordsWithProgressBar(d[1], attributes, i / (len(data) - 1))\n",
    "                       for i,d in enumerate(data)])\n",
    "    df.insert(0, 'isSpam', [d[0] for d in data])\n",
    "    return df\n",
    "\n",
    "def getBernoulliDataFrame(data, allUniqueWords):\n",
    "    print \"Creating DataFrame with Bernoulli model as the feature...\"\n",
    "    attributes = set(allUniqueWords)\n",
    "    df = pd.DataFrame([getBernoulliWithProgressBar(d[1], attributes, i / (len(data) - 1))\n",
    "                       for i,d in enumerate(data)])\n",
    "    df.insert(0, 'isSpam', [d[0] for d in data])\n",
    "    return df\n",
    "\n",
    "def getNaiveBayesPredictions(Test, Train):\n",
    "    return Test.apply(lambda x: naiveBayesOnModel(x['text'], Train), axis=1)\n",
    "\n",
    "def getAccuracyOnNaiveBayes(Test, Train):\n",
    "    return sum(Test.apply(lambda x: naiveBayesOnModel(x['text'], Train) == x['isSpam'], axis=1)) / len(Test)    \n",
    "\n",
    "def PredictWithLR(T, W):\n",
    "    bias = W[0]\n",
    "    PY_1 = 1 / (1 + math.exp(bias + \n",
    "                             T.apply(lambda x: (T[x].sum() / T.sum().sum()) * W[x]).sum()))\n",
    "    PY_0 = 1 - PY_1\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbYIsZero(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "def getProbYIsOne(scores):\n",
    "    return 1 - getProbYIsZero(scores)\n",
    "\n",
    "def getWeight(W, T):\n",
    "    predictions = getPredictions(W, T)\n",
    "    target      = T['isSpam']\n",
    "    attributes  = T.drop('isSpam', axis=1)\n",
    "    attributes.insert(0, 'x_0', 1)\n",
    "    gradient    = np.dot(attributes.T, target - predictions)\n",
    "    return gradient.astype(np.float64)\n",
    "\n",
    "def getLogLikelihood(W, T):\n",
    "    target = T['isSpam']\n",
    "    features = T.drop('isSpam', axis=1)\n",
    "    features.insert(0, 'isSpam', 1)\n",
    "    scores = np.dot(features, W)\n",
    "    return np.sum(target * scores - np.log(1 + np.exp(-scores)))\n",
    "\n",
    "def getPredictions(W, T):\n",
    "    features = T.drop('isSpam', axis=1)\n",
    "    features.insert(0, 'w_0', 1)\n",
    "    return getProbYIsZero(np.dot(T,W)).astype(np.float64)\n",
    "    \n",
    "def getAccuracyOnLR(W, T):\n",
    "    return np.sum([T['isSpam'][i] == prediction.round() for i, prediction in enumerate(getPredictions(W,T))]) / len(T)\n",
    "\n",
    "def splitDataFrame(D, frac):\n",
    "    return (D[0: int(math.floor(len(D) * frac))], D[int(math.floor(len(D) * frac)): len(D)])\n",
    "\n",
    "def L2Regularization(W, V, penalty):\n",
    "    target = V['isSpam']\n",
    "    predictions = getPredictions(W, V)\n",
    "    features = V.drop('isSpam', axis=1)\n",
    "    features.insert(0,'x_0', 1)\n",
    "    gradient = np.dot(features.T, target - predictions)\n",
    "    return (gradient - penalty * W)\n",
    "    \n",
    "def logisticRegression(D, numSteps, learningRate, penalty):\n",
    "    W = np.zeros(len(D.columns))\n",
    "    ham1, ham2   = splitDataFrame(D[D['isSpam'] == 0], 0.7)\n",
    "    spam1, spam2 = splitDataFrame(D[D['isSpam'] == 1], 0.7)\n",
    "    T = ham1.append(spam1).reset_index(drop=True)\n",
    "    V = ham2.append(spam2).reset_index(drop=True)\n",
    "    \n",
    "    print \"Performing gradient descent with weights starting at 0\"\n",
    "    for i in range(1, numSteps):\n",
    "        W += learningRate * getWeight(W,T)\n",
    "        update_progress( round(i / (numSteps - 1), 3))\n",
    "    print \"Regularizing weights using L2 Regularization\"\n",
    "    for i in range(1, numSteps):\n",
    "        W += learningRate * L2Regularization(W, V, penalty)\n",
    "        update_progress( round(i / (numSteps - 1), 3))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHamData  = [[0,f] for f in getDirectoryContents(directory + \"train/ham/\")]\n",
    "trainSpamData = [[1,f] for f in getDirectoryContents(directory + \"train/spam/\")]\n",
    "allTrainData  = trainHamData + trainSpamData\n",
    "testHamData   = [[0,f] for f in getDirectoryContents(directory + \"test/ham/\")]\n",
    "testSpamData  = [[1,f] for f in getDirectoryContents(directory + \"test/spam/\")]\n",
    "allTestData   = pd.DataFrame(testHamData + testSpamData).rename(columns={0: 'isSpam', 1: 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform all files into a single string.\n",
    "allTrainWords = ''.join([f[1] for f in allTrainData])\n",
    "#Retrieve all unique WORDS - Remove all words with numbers/punctuation and replace with space.\n",
    "allUniqueWords = np.unique(getAllWordsFromString(allTrainWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrame with Bernoulli model as the feature...\n",
      "Percent: [##########] 100%  \n",
      "Done...\n",
      "Creating DataFrame with Bernoulli model as the feature...\n",
      "Percent: [##########] 100%  \n",
      "Done...\n",
      "Creating DataFrame with Bag Of Words as the feature...\n",
      "Percent: [##########] 100%  \n",
      "Done...\n",
      "Creating DataFrame with Bag Of Words as the feature...\n",
      "Percent: [##########] 100%  \n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "#Get a dataframe with bernoulli as the feature\n",
    "trainB = getBernoulliDataFrame(allTrainData, allUniqueWords)\n",
    "testB  = getBernoulliDataFrame(testHamData + testSpamData, allUniqueWords)\n",
    "#Get a dataframe with bag of words as a feature for training\n",
    "trainBOW = getBagOfWordsDataFrame(allTrainData, allUniqueWords)\n",
    "testBOW  = getBagOfWordsDataFrame(testHamData + testSpamData, allUniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(Y = 0 | X)\n",
    "def getProbYIsZero(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "# P(Y = 1 | X)\n",
    "def getProbYIsOne(scores):\n",
    "    return 1 - getProbYIsZero(scores)\n",
    "\n",
    "# Returns weight after gradient descent without learning rate.\n",
    "def getWeight(W, T):\n",
    "    predictions = getPredictions(W, T)\n",
    "    target      = T['isSpam']\n",
    "    attributes  = T.drop('isSpam', axis=1)\n",
    "    attributes.insert(0, 'x_0', 1)\n",
    "    gradient    = np.dot(attributes.T, target - predictions)\n",
    "    return gradient.astype(np.float64)\n",
    "\n",
    "# Return log likelihood on dataset\n",
    "def getLogLikelihood(W, T):\n",
    "    target = T['isSpam']\n",
    "    features = T.drop('isSpam', axis=1)\n",
    "    features.insert(0, 'isSpam', 1)\n",
    "    scores = np.dot(features, W)\n",
    "    return np.sum(target * scores - np.log(1 + np.exp(-scores)))\n",
    "\n",
    "# Return all predictions before threshold\n",
    "def getLRPredictions(W, T):\n",
    "    features = T.drop('isSpam', axis=1)\n",
    "    features.insert(0, 'w_0', 1)\n",
    "    return getProbYIsZero(np.dot(T,W)).astype(np.float64)\n",
    "    \n",
    "# Returns \n",
    "def getAccuracyOnLR(W, T):\n",
    "    return np.sum([T['isSpam'][i] == prediction.round() for i, prediction in enumerate(getPredictions(W,T))]) / len(T)\n",
    "\n",
    "def splitDataFrame(D, frac):\n",
    "    return (D[0: int(math.floor(len(D) * frac))], D[int(math.floor(len(D) * frac)): len(D)])\n",
    "\n",
    "def L2Regularization(W, V, penalty):\n",
    "    target = V['isSpam']\n",
    "    predictions = getPredictions(W, V)\n",
    "    features = V.drop('isSpam', axis=1)\n",
    "    features.insert(0,'x_0', 1)\n",
    "    gradient = np.dot(features.T, target - predictions)\n",
    "    return (gradient - penalty * W)\n",
    "    \n",
    "def logisticRegression(D, numSteps, learningRate, penalty):\n",
    "    W = np.zeros(len(D.columns))\n",
    "    ham1, ham2   = splitDataFrame(D[D['isSpam'] == 0], 0.7)\n",
    "    spam1, spam2 = splitDataFrame(D[D['isSpam'] == 1], 0.7)\n",
    "    T = ham1.append(spam1).reset_index(drop=True)\n",
    "    V = ham2.append(spam2).reset_index(drop=True)\n",
    "    \n",
    "    print \"Performing gradient descent with weights starting at 0\"\n",
    "    for i in range(1, numSteps):\n",
    "        W += learningRate * getWeight(W,T)\n",
    "        update_progress( round(i / (numSteps - 1), 3))\n",
    "    print \"Regularizing weights using L2 Regularization\"\n",
    "    for i in range(1, numSteps):\n",
    "        W += learningRate * L2Regularization(W, V, penalty)\n",
    "        update_progress( round(i / (numSteps - 1), 3))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with SGDClassifier using Bag of Words as features:\n",
      "Accuracy : 0.933054393305\n",
      "Precision: 0.871212121212\n",
      "Recall   : 0.257847533632\n",
      "F1       : 0.397923875433\n",
      "Performance with SGDClassifier using Bernoulli as features:\n",
      "Accuracy : 0.949790794979\n",
      "Precision: 0.884057971014\n",
      "Recall   : 0.26872246696\n",
      "F1       : 0.412162162162\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "X = trainBOW.drop('isSpam', axis=1)\n",
    "Y = trainBOW['isSpam']\n",
    "clfBOW = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "clfBOW.fit(X,Y)\n",
    "\n",
    "X = trainB.drop('isSpam', axis=1)\n",
    "Y = trainB['isSpam']\n",
    "clfBer = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "clfBer.fit(X,Y)\n",
    "\n",
    "SGDBOWpredictions   = clfBOW.predict(testBOW.drop('isSpam', axis=1))\n",
    "SGDBerpredictions   = clfBer.predict(testB.drop('isSpam', axis=1))\n",
    "\n",
    "printAccPrecRecallF1(target, SGDBOWpredictions, \"SGDClassifier\", \"Bag of Words\")\n",
    "printAccPrecRecallF1(target, SGDBerpredictions, \"SGDClassifier\", \"Bernoulli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccPrecRecallF1(Test, predictions):\n",
    "    positives = testBOW[testBOW['isSpam'] == 1]['isSpam'].index\n",
    "    negatives = testBOW[testBOW['isSpam'] == 0]['isSpam'].index\n",
    "    true_positives  = sum(predictions[positives] == testBOW['isSpam'][positives])\n",
    "    false_positives = sum(predictions[positives] != testBOW['isSpam'][positives])\n",
    "    false_negatives = sum(predictions[negatives] == testBOW['isSpam'][negatives])\n",
    "    false_positives = sum(predictions[negatives] != testBOW['isSpam'][negatives])\n",
    "    \n",
    "    accuracy        = sum(predictions == testBOW['isSpam']) / len(predictions)\n",
    "    precision       = true_positives / (true_positives + false_positives)\n",
    "    recall          = true_positives / (true_positives + false_negatives)\n",
    "    F1              = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return (accuracy, precision, recall, F1)\n",
    "\n",
    "def printAccPrecRecallF1(Test, predictions, algorithmName, featureType):\n",
    "    acc, prec, recall, F1 = getAccPrecRecallF1(Test, predictions)\n",
    "    print \"Performance with \" + algorithmName + \" using \" + featureType + \" as features:\"\n",
    "    print \"Accuracy : \" + str(acc)\n",
    "    print \"Precision: \" + str(prec)\n",
    "    print \"Recall   : \" + str(recall)\n",
    "    print \"F1       : \" + str(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient descent with weights starting at 0\n",
      "Percent: [#########-] 86.5% "
     ]
    }
   ],
   "source": [
    "WBOW = logisticRegression(trainBOW, 10000, 0.001, 0.1)\n",
    "WBer = logisticRegression(trainB,   1000, 0.001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-9de4214bd899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmultipredictions\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mgetNaiveBayesPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallTestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainBOW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdiscretepredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNaiveBayesPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallTestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mLRBOWpredictions\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mgetLRPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWBOW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestBOW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLRBerpredictions\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mgetLRPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWBer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mSGDBOWpredictions\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mclfBOW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestBOW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'isSpam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3b6e20f90640>\u001b[0m in \u001b[0;36mgetNaiveBayesPredictions\u001b[0;34m(Test, Train)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetNaiveBayesPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnaiveBayesOnModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetAccuracyOnNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6485\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6486\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/pandas/core/apply.pyc\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/pandas/core/apply.pyc\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m                                           \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                                           \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                                           labels=labels)\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.reduce\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3b6e20f90640>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetNaiveBayesPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnaiveBayesOnModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetAccuracyOnNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3b6e20f90640>\u001b[0m in \u001b[0;36mnaiveBayesOnModel\u001b[0;34m(text, T)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnaiveBayesOnModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mp_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'isSpam'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgetProductOfProbabities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'isSpam'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'isSpam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mp_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'isSpam'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgetProductOfProbabities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'isSpam'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'isSpam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp_0\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mp_1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3b6e20f90640>\u001b[0m in \u001b[0;36mgetProductOfProbabities\u001b[0;34m(text, T)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# works with both bernoulli and bag of words model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetProductOfProbabities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfeatureSums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetAllWordsFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtotalWords\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatureSums\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotalWords\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  10929\u001b[0m                                       skipna=skipna, min_count=min_count)\n\u001b[1;32m  10930\u001b[0m         return self._reduce(f, name, axis=axis, skipna=skipna,\n\u001b[0;32m> 10931\u001b[0;31m                             numeric_only=numeric_only, min_count=min_count)\n\u001b[0m\u001b[1;32m  10932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10933\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   7405\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7406\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7407\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7409\u001b[0m                 if (filter_type == 'bool' and is_object_dtype(values) and\n",
      "\u001b[0;32m/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   7394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7395\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7396\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7398\u001b[0m         \u001b[0;31m# exclude timedelta/datetime unless we are uniform types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/pandas/core/nanops.pyc\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/pandas/core/nanops.pyc\u001b[0m in \u001b[0;36mnansum\u001b[0;34m(values, axis, skipna, min_count, mask)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \"\"\"\n\u001b[1;32m    428\u001b[0m     values, mask, dtype, dtype_max, _ = _get_values(values,\n\u001b[0;32m--> 429\u001b[0;31m                                                     skipna, 0, mask=mask)\n\u001b[0m\u001b[1;32m    430\u001b[0m     \u001b[0mdtype_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_float_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/.conda/envs/csgrads1/lib/python2.7/site-packages/pandas/core/nanops.pyc\u001b[0m in \u001b[0;36m_get_values\u001b[0;34m(values, skipna, fill_value, fill_value_typ, isfinite, copy, mask)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "multipredictions    = getNaiveBayesPredictions(Test=allTestData, Train=trainBOW)\n",
    "discretepredictions = getNaiveBayesPredictions(Test=allTestData, Train=trainB)\n",
    "LRBOWpredictions    = getLRPredictions(WBOW, testBOW).round()\n",
    "LRBerpredictions    = getLRPredictions(WBer, testB).round()\n",
    "SGDBOWpredictions   = clfBOW.predict(testBOW.drop('isSpam', axis=1))\n",
    "SGDBerpredictions   = clfBer.predict(testB.drop('isSpam', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with Multinomial Bayes using Bag of Words as features:\n",
      "Accuracy : 0.941422594142\n",
      "Precision: 0.939655172414\n",
      "Recall   : 0.242222222222\n",
      "F1       : 0.385159010601\n",
      "Performance with Discrete Bayes using Bernoulli as features:\n",
      "Accuracy : 0.86820083682\n",
      "Precision: 0.958904109589\n",
      "Recall   : 0.168674698795\n",
      "F1       : 0.286885245902\n",
      "Performance with Logistic Regression using Bag of Words as features:\n",
      "Accuracy : 0.94769874477\n",
      "Precision: 0.888888888889\n",
      "Recall   : 0.264900662252\n",
      "F1       : 0.408163265306\n",
      "Performance with Logistic Regression using Bernoulli as features:\n",
      "Accuracy : 0.956066945607\n",
      "Precision: 0.929133858268\n",
      "Recall   : 0.258205689278\n",
      "F1       : 0.404109589041\n",
      "Performance with SGDClassifier using Bag of Words as features:\n",
      "Accuracy : 0.926778242678\n",
      "Precision: 0.862595419847\n",
      "Recall   : 0.255079006772\n",
      "F1       : 0.393728222997\n",
      "Performance with SGDClassifier using Bernoulli as features:\n",
      "Accuracy : 0.926778242678\n",
      "Precision: 0.862595419847\n",
      "Recall   : 0.255079006772\n",
      "F1       : 0.393728222997\n"
     ]
    }
   ],
   "source": [
    "target = trainBOW\n",
    "printAccPrecRecallF1(target, multipredictions, \"Multinomial Bayes\", \"Bag of Words\")\n",
    "printAccPrecRecallF1(target, discretepredictions, \"Discrete Bayes\", \"Bernoulli\")\n",
    "printAccPrecRecallF1(target, LRBOWpredictions, \"Logistic Regression\", \"Bag of Words\")\n",
    "printAccPrecRecallF1(target, LRBerpredictions, \"Logistic Regression\", \"Bernoulli\")\n",
    "printAccPrecRecallF1(target, SGDBOWpredictions, \"SGDClassifier\", \"Bag of Words\")\n",
    "printAccPrecRecallF1(target, SGDBerpredictions, \"SGDClassifier\", \"Bernoulli\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
