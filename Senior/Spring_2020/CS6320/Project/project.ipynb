{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "trainFile = 'train.xml'\n",
    "testFile = 'test.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Dataset(TensorDataset):\n",
    "    def __init__(self, p=None, h=None, l=None):\n",
    "        if p is None and h is None and l is None :\n",
    "            return\n",
    "        self.p = torch.from_numpy(np.array(p))\n",
    "        self.h = torch.from_numpy(np.array(h))\n",
    "        labels = np.array([[1 if i == 1 else 0, 1 if i ==2 else 0] for i in l])\n",
    "        self.labels = torch.from_numpy(labels).type(torch.FloatTensor)\n",
    "    \n",
    "    def to(self, device):\n",
    "        newDataset = Dataset()\n",
    "        newDataset.p = self.p.to(device)\n",
    "        newDataset.h = self.h.to(device)\n",
    "        newDataset.labels = self.labels.to(device)\n",
    "        return newDataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.p)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return [self.p[item], self.h[item]], self.labels[item]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_length, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_length, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        n = len(data[0])\n",
    "        et = self.embedding(data[0])\n",
    "        ht = self.embedding(data[1])\n",
    "        rt, _ = self.lstm(et.view(n,1,-1))\n",
    "        rh, _ = self.lstm(ht.view(n,1,-1))\n",
    "        rth = torch.cat((rt.view(n,-1),rh.view(n,-1)), dim=0)\n",
    "        fc = self.fc(torch.sum(self.dropout(rth),dim=0))\n",
    "        return F.softmax(fc, dim=0)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class ModelTrainer():\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion.to(device)\n",
    "        \n",
    "    def train(self, trainData, testData, trainSample, testSample, N_EPOCHS=10):\n",
    "        trainData = trainData.to(device)\n",
    "        testData = testData.to(device)\n",
    "        N_EPOCHS = N_EPOCHS\n",
    "        best_valid_loss = float('inf')\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            t = time.time()\n",
    "            train_loss, train_acc = self.trainModel(trainData, trainSample)\n",
    "            valid_loss, valid_acc = self.evaluateModel(testData, testSample)\n",
    "            epoch_secs = time.time() - t            \n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_secs:.2f}s')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        return model\n",
    "\n",
    "    def trainModel(self, data, sampler):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for i in sampler:\n",
    "            optimizer.zero_grad()\n",
    "            batch_data = data[i][0]\n",
    "            batch_label = data[i][1]\n",
    "            predictions = self.model(batch_data)\n",
    "            loss = self.criterion(predictions, batch_label)\n",
    "            acc = self.accuracy(predictions, batch_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        return epoch_loss / len(sampler), epoch_acc / len(sampler)\n",
    "    \n",
    "    def evaluateModel(self, data, sampler):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in sampler:\n",
    "                batch_data = data[i][0]\n",
    "                batch_label = data[i][1]\n",
    "                predictions = self.model(batch_data)\n",
    "                loss = self.criterion(predictions, batch_label)\n",
    "                acc = self.accuracy(batch_label, predictions)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "            \n",
    "        return epoch_loss / len(sampler), epoch_acc / len(sampler)\n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        y_pred = y_pred.data.argmax()\n",
    "        y_true = y_true.data.argmax()\n",
    "        return (y_true == y_pred).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class XMLToTensor:\n",
    "    def __init__(self, files, maxLength=None):\n",
    "        data = np.array([self.parseXML(file) for file in files])\n",
    "        self.p = np.array([p for f in data for p in f[0]])\n",
    "        self.h = np.array([p for f in data for p in f[1]])\n",
    "        self.l = np.array([p for f in data for p in f[2]])\n",
    "        self.wd = self.createWordToIntegerDict([word for words in self.p for word in words] + \n",
    "                                               [word for words in self.h for word in words])\n",
    "        self.ld = self.createWordToIntegerDict([label for label in self.l])\n",
    "        if maxLength is None:\n",
    "            self.maxLength = max([len(s) for s in self.p + self.h])\n",
    "        else:\n",
    "            self.maxLength = maxLength\n",
    "        \n",
    "    def getTensor(self):\n",
    "        p,h,l = self.prepareDataset(self.p,self.h,self.l)\n",
    "        return Dataset(p,h,l)\n",
    "    \n",
    "    def splitTensor(self, trainSplit, testSplit):\n",
    "        if trainSplit + testSplit != 1:\n",
    "            print('Invalid split')\n",
    "            return None\n",
    "        n = len(self.p)\n",
    "        p,h,l = self.prepareDataset(self.p,self.h,self.l)\n",
    "        splitI = int(trainSplit * n)\n",
    "        return Dataset(p[0:splitI],h[0:splitI],l[0:splitI]), Dataset(p[splitI:n], h[splitI:n], l[splitI:n])\n",
    "        \n",
    "    def getWordDictionary(self):\n",
    "        return self.wd\n",
    "    \n",
    "    def getLabelDictionary(self):\n",
    "        return self.ld\n",
    "    \n",
    "    def parseXML(self, file):\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        pairs = root.findall('./pair')\n",
    "        p = np.array([re.sub(\"[^\\w]\", \" \", pair.find('./t').text.lower()).split() for pair in pairs])\n",
    "        h = np.array([re.sub(\"[^\\w]\", \" \", pair.find('./h').text.lower()).split() for pair in pairs])\n",
    "        l = np.array([pair.get('value') for pair in pairs])\n",
    "        return p,h,l\n",
    "    \n",
    "    def createWordToIntegerDict(self,words):\n",
    "        d = defaultdict(int)\n",
    "        i = 0\n",
    "        for word in words:\n",
    "            if d[word] == 0:\n",
    "                i += 1\n",
    "                d[word] = i\n",
    "        return d\n",
    "    \n",
    "    def transformWordsToIntegers(self,w, d):\n",
    "        return [[d[word] for word in words] for words in w]\n",
    "        \n",
    "    def transformListsToUniformLength(self,w, padding=0):\n",
    "        L = [[padding for _ in range(self.maxLength)] for _ in w]\n",
    "        for i,l in enumerate(w):\n",
    "            end = len(l) if len(l) <= self.maxLength else self.maxLength\n",
    "            L[i][0:end] = l[0:end]\n",
    "        return L\n",
    "        \n",
    "    def prepareDataset(self, p, h, l):\n",
    "        p = self.transformListsToUniformLength(self.transformWordsToIntegers(p, self.wd))\n",
    "        h = self.transformListsToUniformLength(self.transformWordsToIntegers(h, self.wd))\n",
    "        l = [self.ld[l] for l in l]\n",
    "        return p,h,l\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "data = XMLToTensor([trainFile, testFile], maxLength=30)\n",
    "train, test = data.splitTensor(0.8,0.2)\n",
    "wordDict = data.getWordDictionary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNN(len(wordDict) + 1, 30, 30, 2).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "train = train.to(device)\n",
    "test = test.to(device)\n",
    "\n",
    "trainSample, testSample = RandomSampler(train), SequentialSampler(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch: 01 | Epoch Time: 4.62s\n",
      "\tTrain Loss: 1.311 | Train Acc: 49.31%\n",
      "\t Val. Loss: 0.817 |  Val. Acc: 45.26%\n",
      "Epoch: 02 | Epoch Time: 4.57s\n",
      "\tTrain Loss: 1.142 | Train Acc: 48.76%\n",
      "\t Val. Loss: 0.822 |  Val. Acc: 50.00%\n",
      "Epoch: 03 | Epoch Time: 4.50s\n",
      "\tTrain Loss: 1.062 | Train Acc: 48.12%\n",
      "\t Val. Loss: 0.834 |  Val. Acc: 51.82%\n",
      "Epoch: 04 | Epoch Time: 4.53s\n",
      "\tTrain Loss: 0.958 | Train Acc: 50.96%\n",
      "\t Val. Loss: 0.735 |  Val. Acc: 48.18%\n",
      "Epoch: 05 | Epoch Time: 4.53s\n",
      "\tTrain Loss: 0.915 | Train Acc: 51.88%\n",
      "\t Val. Loss: 0.728 |  Val. Acc: 51.09%\n",
      "Epoch: 06 | Epoch Time: 4.50s\n",
      "\tTrain Loss: 0.832 | Train Acc: 52.88%\n",
      "\t Val. Loss: 0.730 |  Val. Acc: 50.73%\n",
      "Epoch: 07 | Epoch Time: 4.56s\n",
      "\tTrain Loss: 0.797 | Train Acc: 53.34%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 50.00%\n",
      "Epoch: 08 | Epoch Time: 4.55s\n",
      "\tTrain Loss: 0.786 | Train Acc: 54.16%\n",
      "\t Val. Loss: 0.712 |  Val. Acc: 53.65%\n",
      "Epoch: 09 | Epoch Time: 4.53s\n",
      "\tTrain Loss: 0.749 | Train Acc: 55.26%\n",
      "\t Val. Loss: 0.706 |  Val. Acc: 55.11%\n",
      "Epoch: 10 | Epoch Time: 4.50s\n",
      "\tTrain Loss: 0.718 | Train Acc: 55.99%\n",
      "\t Val. Loss: 0.704 |  Val. Acc: 51.82%\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "RNN(\n  (embedding): Embedding(7559, 30)\n  (lstm): LSTM(30, 30, bidirectional=True)\n  (dropout): Dropout(p=0.8, inplace=False)\n  (fc): Linear(in_features=60, out_features=2, bias=True)\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "trainer = ModelTrainer(model, optimizer, criterion, device)\n",
    "trainer.train(train, test, trainSample, testSample, N_EPOCHS=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-09d14b54fdb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: squeeze() received an invalid combination of arguments - got (tuple), but expected one of:\n * ()\n      didn't match because some of the arguments have invalid types: (!tuple!)\n * (name dim)\n      didn't match because some of the arguments have invalid types: (!tuple!)\n * (int dim)\n      didn't match because some of the arguments have invalid types: (!tuple!)\n"
     ],
     "ename": "TypeError",
     "evalue": "squeeze() received an invalid combination of arguments - got (tuple), but expected one of:\n * ()\n      didn't match because some of the arguments have invalid types: (!tuple!)\n * (name dim)\n      didn't match because some of the arguments have invalid types: (!tuple!)\n * (int dim)\n      didn't match because some of the arguments have invalid types: (!tuple!)\n",
     "output_type": "error"
    }
   ],
   "source": [
    "torch.randn(10,10).squeeze((1,1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}