{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def reportPerformance(y_pred, test_labels, description):                                                                                                                                                                                \n",
    "    print(description)                                                                                                                                                                                                                  \n",
    "    print('Accuracy:  ', accuracy_score(test_labels, y_pred), '\\n',                                                                                                                                                                     \n",
    "          'Precision: ', precision_score(test_labels, y_pred), '\\n',                                                                                                                                                                    \n",
    "          'Recall:    ', recall_score(test_labels, y_pred), '\\n',                                                                                                                                                                       \n",
    "          'F-Score:   ', f1_score(test_labels, y_pred), '\\n', sep='')\n",
    "\n",
    "trainFile = 'train.xml'\n",
    "testFile = 'test.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Dataset(TensorDataset):\n",
    "    def __init__(self, p=None, h=None, l=None):\n",
    "        if p is None and h is None and l is None :\n",
    "            return\n",
    "        self.p = torch.from_numpy(np.array(p))\n",
    "        self.h = torch.from_numpy(np.array(h))\n",
    "        labels = np.array([[1 if i == 1 else 0, 1 if i ==2 else 0] for i in l])\n",
    "        self.labels = torch.from_numpy(labels).type(torch.FloatTensor)\n",
    "    \n",
    "    def to(self, device):\n",
    "        newDataset = Dataset()\n",
    "        newDataset.p = self.p.to(device)\n",
    "        newDataset.h = self.h.to(device)\n",
    "        newDataset.labels = self.labels.to(device)\n",
    "        return newDataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.p)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        n = len(self.p[item])\n",
    "        return torch.cat((self.p[item].view(-1,n), self.h[item].view(-1,n))), self.labels[item]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_length, embedding_dim, hidden_dim, output_dim, lstm_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_length, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.lstmL = []\n",
    "        current_dim = hidden_dim * 2\n",
    "        for i in range(lstm_layers):\n",
    "            self.lstmL.append(nn.LSTM(current_dim, hidden_dim))\n",
    "            current_dim = hidden_dim\n",
    "        self.lstmL = nn.ModuleList(self.lstmL)\n",
    "        self.fc = nn.Linear(current_dim, output_dim)\n",
    "        self.n = embedding_dim\n",
    "        \n",
    "    def forward(self, p, h):\n",
    "        n = len(p)\n",
    "        et = self.embedding(p)\n",
    "        ht = self.embedding(h)\n",
    "        rt, _ = self.lstm(et.view(n,1,-1))\n",
    "        rh, _ = self.lstm(ht.view(n,1,-1))\n",
    "        for lstm in self.lstmL:\n",
    "            rt, _ = lstm(rt)\n",
    "            rh, _ = lstm(rh)\n",
    "        rth = torch.cat((rt.view(self.n,-1),rh.view(self.n,-1)), dim=0)\n",
    "        fc = self.fc(torch.sum(rth,dim=0))\n",
    "        return F.softmax(fc,dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ModelTools():\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, trainData, testData, N_EPOCHS=10):\n",
    "        N_EPOCHS = N_EPOCHS\n",
    "        best_valid_loss = float('inf')\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            t = time.time()\n",
    "            train_loss, train_acc = self.trainModel(trainData)\n",
    "            valid_loss, valid_acc = self.evaluateModel(testData)\n",
    "            epoch_secs = time.time() - t\n",
    "            if valid_loss < best_valid_loss:\n",
    "                torch.save(model.state_dict(), 'best_model.h2')\n",
    "                best_valid_loss = valid_loss\n",
    "            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_secs:.2f}s')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        return model\n",
    "\n",
    "    def trainModel(self, data):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for d in data:\n",
    "            optimizer.zero_grad()\n",
    "            p = [i[0] for i in d[0]]\n",
    "            h = [i[1] for i in d[0]]\n",
    "            batch_labels = torch.cat([i.unsqueeze(0) for i in d[1]])\n",
    "            predictions = torch.cat([self.model(p[i], h[i]).unsqueeze(0) for i in range(len(p))])\n",
    "            loss = self.criterion(predictions, batch_labels)\n",
    "            acc = self.accuracy(predictions, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        return epoch_loss / len(data), epoch_acc / len(data)\n",
    "    \n",
    "    def evaluateModel(self, data):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for d in data:\n",
    "                p = [i[0] for i in d[0]]\n",
    "                h = [i[1] for i in d[0]]\n",
    "                batch_labels = torch.cat([i.unsqueeze(0) for i in d[1]])\n",
    "                predictions = torch.cat([self.model(p[i], h[i]).unsqueeze(0) for i in range(len(p))])   \n",
    "                loss = self.criterion(predictions, batch_labels)\n",
    "                acc = self.accuracy(batch_labels, predictions)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "            \n",
    "        return epoch_loss / len(data), epoch_acc / len(data)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        self.model.eval()\n",
    "        predictions = torch.Tensor().type(torch.LongTensor).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            for d in data:\n",
    "                p = [i[0] for i in d[0]]\n",
    "                h = [i[1] for i in d[0]]\n",
    "                output = torch.cat([self.model(p[i], h[i]).argmax().view(1,-1) + 1 for i in range(len(p))])\n",
    "                predictions = torch.cat((predictions,output))\n",
    "        return torch.flatten(predictions).cpu().detach()\n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        a = y_true.argmax(dim=1)\n",
    "        b = y_pred.argmax(dim=1)\n",
    "        return torch.sum(torch.eq(a,b)).type(torch.FloatTensor) / len(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class XMLToTensor:\n",
    "    def __init__(self, files, maxLength=None):\n",
    "        data = np.array([self.parseXML(file) for file in files])\n",
    "        self.p = np.array([p for f in data for p in f[0]])\n",
    "        self.h = np.array([p for f in data for p in f[1]])\n",
    "        self.l = np.array([p for f in data for p in f[2]])\n",
    "        self.wd = self.createWordToIntegerDict([word for words in self.p for word in words] + \n",
    "                                               [word for words in self.h for word in words])\n",
    "        self.ld = self.createWordToIntegerDict([label for label in self.l])\n",
    "        if maxLength is None:\n",
    "            self.maxLength = max([len(s) for s in self.p + self.h])\n",
    "        else:\n",
    "            self.maxLength = maxLength\n",
    "        \n",
    "    def getTensor(self):\n",
    "        p,h,l = self.prepareDataset(self.p,self.h,self.l)\n",
    "        return Dataset(p,h,l)\n",
    "    \n",
    "    def splitTensor(self, trainSplit, testSplit):\n",
    "        if trainSplit + testSplit != 1:\n",
    "            print('Invalid split')\n",
    "            return None\n",
    "        n = len(self.p)\n",
    "        p,h,l = self.prepareDataset(self.p,self.h,self.l)\n",
    "        splitI = int(trainSplit * n)\n",
    "        return Dataset(p[0:splitI],h[0:splitI],l[0:splitI]), Dataset(p[splitI:n], h[splitI:n], l[splitI:n])\n",
    "        \n",
    "    def getWordDictionary(self):\n",
    "        return self.wd\n",
    "    \n",
    "    def getLabelDictionary(self):\n",
    "        return self.ld\n",
    "    \n",
    "    def parseXML(self, file):\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        pairs = root.findall('./pair')\n",
    "        p = np.array([re.sub(\"[^\\w]\", \" \", pair.find('./t').text.lower()).split() for pair in pairs])\n",
    "        h = np.array([re.sub(\"[^\\w]\", \" \", pair.find('./h').text.lower()).split() for pair in pairs])\n",
    "        l = np.array([pair.get('value') for pair in pairs])\n",
    "        return p,h,l\n",
    "    \n",
    "    def createWordToIntegerDict(self,words):\n",
    "        d = defaultdict(int)\n",
    "        i = 0\n",
    "        for word in words:\n",
    "            if d[word] == 0:\n",
    "                i += 1\n",
    "                d[word] = i\n",
    "        return d\n",
    "    \n",
    "    def transformWordsToIntegers(self,w, d):\n",
    "        return [[d[word] for word in words] for words in w]\n",
    "        \n",
    "    def transformListsToUniformLength(self,w, padding=0):\n",
    "        L = [[padding for _ in range(self.maxLength)] for _ in w]\n",
    "        for i,l in enumerate(w):\n",
    "            end = len(l) if len(l) <= self.maxLength else self.maxLength\n",
    "            L[i][0:end] = l[0:end]\n",
    "        return L\n",
    "        \n",
    "    def prepareDataset(self, p, h, l):\n",
    "        p = self.transformListsToUniformLength(self.transformWordsToIntegers(p, self.wd))\n",
    "        h = self.transformListsToUniformLength(self.transformWordsToIntegers(h, self.wd))\n",
    "        l = [self.ld[l] for l in l]\n",
    "        return p,h,l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data = XMLToTensor([trainFile, testFile], maxLength=10)\n",
    "maxLength = data.maxLength\n",
    "train, test = data.splitTensor(0.9,0.1)\n",
    "wordDict = data.getWordDictionary()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNN(len(wordDict) + 1, maxLength, 30, 2, 3).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "train = train.to(device)\n",
    "test = test.to(device)\n",
    "trainSampler = RandomSampler(train)\n",
    "testSampler = SequentialSampler(test)\n",
    "train = DataLoader(train, batch_size=1, sampler=trainSampler)\n",
    "test = DataLoader(test, batch_size=1, sampler=testSampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The model has 111,652 trainable parameters\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch: 01 | Epoch Time: 6.54s\n",
      "\tTrain Loss: 0.710 | Train Acc: 47.64%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 50.36%\n",
      "Epoch: 02 | Epoch Time: 6.83s\n",
      "\tTrain Loss: 0.702 | Train Acc: 51.95%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 50.36%\n",
      "Epoch: 03 | Epoch Time: 6.54s\n",
      "\tTrain Loss: 0.704 | Train Acc: 50.24%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 48.91%\n",
      "Epoch: 04 | Epoch Time: 6.55s\n",
      "\tTrain Loss: 0.702 | Train Acc: 50.57%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.36%\n",
      "Epoch: 05 | Epoch Time: 6.55s\n",
      "\tTrain Loss: 0.700 | Train Acc: 50.98%\n",
      "\t Val. Loss: 0.701 |  Val. Acc: 50.36%\n",
      "Epoch: 06 | Epoch Time: 6.55s\n",
      "\tTrain Loss: 0.702 | Train Acc: 48.62%\n",
      "\t Val. Loss: 0.699 |  Val. Acc: 49.64%\n",
      "Epoch: 07 | Epoch Time: 6.53s\n",
      "\tTrain Loss: 0.700 | Train Acc: 49.59%\n",
      "\t Val. Loss: 0.717 |  Val. Acc: 49.64%\n",
      "Epoch: 08 | Epoch Time: 6.54s\n",
      "\tTrain Loss: 0.700 | Train Acc: 48.21%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 49.64%\n",
      "Epoch: 09 | Epoch Time: 6.56s\n",
      "\tTrain Loss: 0.696 | Train Acc: 50.24%\n",
      "\t Val. Loss: 0.737 |  Val. Acc: 50.36%\n",
      "Epoch: 10 | Epoch Time: 6.67s\n",
      "\tTrain Loss: 0.700 | Train Acc: 49.19%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 49.64%\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "RNN(\n  (embedding): Embedding(7559, 30)\n  (lstm): LSTM(30, 30, bidirectional=True)\n  (dropout): Dropout(p=0.8, inplace=False)\n  (fc): Linear(in_features=60, out_features=2, bias=True)\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "trainer = ModelTools(model, optimizer, criterion, device)\n",
    "trainer.train(train, test, N_EPOCHS=10, )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8b6d38265755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxLength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.h2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelTools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ],
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error"
    }
   ],
   "source": [
    "model = RNN(len(wordDict) + 1, maxLength, 30, 2, 1).to(device)\n",
    "model.load_state_dict(torch.load('best_model.h2'))\n",
    "y_pred = ModelTools(model, optimizer, criterion, device).predict(test).numpy()\n",
    "y_true = np.array([1 if i[0] == 1 else 2 for i in test.dataset.labels])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
